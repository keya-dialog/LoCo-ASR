{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578c8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import SpeechT5ForSpeechToText, SpeechT5ForTextToSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device\n",
    "device =torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2596702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_asr = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\").to(device)\n",
    "#model_orig = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\").to(device)\n",
    "model_tts = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "ckpt = torch.load(\"../checkpoints/speecht5_base.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b236844",
   "metadata": {},
   "source": [
    "# TEXT PRE-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39a0951",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speecht5.encoder.prenet.embed_tokens.weight torch.Size([81, 768]) Parameter containing:\n",
      "tensor([[ 0.2236,  0.1462,  0.2703,  ..., -0.5044, -0.3403, -0.2323],\n",
      "        [-0.0267, -0.0211,  0.0806,  ...,  0.0826, -0.0250,  0.0142],\n",
      "        [ 0.1836,  0.1970,  0.1066,  ..., -0.0246,  0.0242, -0.1271],\n",
      "        ...,\n",
      "        [-0.0618, -0.0311,  0.0687,  ...,  0.1088, -0.1075, -0.0759],\n",
      "        [-0.0393, -0.1295,  0.0012,  ...,  0.0482, -0.0222, -0.3157],\n",
      "        [-0.0368, -0.0320,  0.0710,  ...,  0.1121, -0.0775, -0.0605]],\n",
      "       requires_grad=True)\n",
      "speecht5.encoder.prenet.encode_positions.alpha torch.Size([]) Parameter containing:\n",
      "tensor(0.1053, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, p in model_tts.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.prenet\"):\n",
    "        print(name, p.shape ,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74035306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_encoder_prenet.encoder_prenet.0.weight torch.Size([81, 768]) tensor([[ 0.2236,  0.1462,  0.2703,  ..., -0.5044, -0.3403, -0.2323],\n",
      "        [-0.0267, -0.0211,  0.0806,  ...,  0.0826, -0.0250,  0.0142],\n",
      "        [ 0.1836,  0.1970,  0.1066,  ..., -0.0246,  0.0242, -0.1271],\n",
      "        ...,\n",
      "        [-0.0473, -0.0490,  0.0584,  ...,  0.1119, -0.1022, -0.0686],\n",
      "        [-0.0393, -0.1295,  0.0012,  ...,  0.0482, -0.0222, -0.3157],\n",
      "        [-0.0368, -0.0320,  0.0710,  ...,  0.1121, -0.0775, -0.0605]])\n",
      "text_encoder_prenet.encoder_prenet.1.alpha torch.Size([]) tensor(0.1270)\n"
     ]
    }
   ],
   "source": [
    "for name, p in ckpt['model'].items():\n",
    "    if name.startswith(\"text_encoder_prenet\"):\n",
    "        print(name, p.shape, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7055fa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.1053, requires_grad=True),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tts.speecht5.encoder.prenet.encode_positions.alpha, model_tts.speecht5.encoder.prenet.encode_positions.alpha.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f22f321",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_tts\u001b[38;5;241m.\u001b[39mspeecht5\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mprenet\u001b[38;5;241m.\u001b[39mencode_positions\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1270\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/torch/nn/parameter.py:39\u001b[0m, in \u001b[0;36mParameter.__new__\u001b[0;34m(cls, data, requires_grad)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39m_make_subclass(\u001b[38;5;28mcls\u001b[39m, data, requires_grad)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Path for custom tensors: set a flag on the instance to indicate parameter-ness.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating a Parameter from an instance of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires that detach() returns an instance of the same type, but return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(t)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was found instead. To use the type as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter, please correct the detach() semantics defined by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits __torch_dispatch__() implementation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "model_tts.speecht5.encoder.prenet.encode_positions.alpha = torch.nn.Parameter(0.1270, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa6235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee1a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7eeedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51078fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51777f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01c4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa214626",
   "metadata": {},
   "source": [
    "# SPEECH PRE-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9309d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speecht5.encoder.prenet.feature_projection.layer_norm.weight tensor([0.9146, 0.6802, 0.7935, 0.4436, 1.0410], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for name, phf in model_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.prenet.feature_projection.layer_norm.weight\"):\n",
    "        print(name, phf[:5], phf.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d715cfa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech_encoder_prenet.layer_norm.weight tensor([0.9727, 0.7222, 0.8418, 0.4727, 1.1113], device='cuda:0') torch.Size([512])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for name, p in ckpt['model'].items():\n",
    "    if name.startswith(\"speech_encoder_prenet.layer_norm.weight\"):\n",
    "        print(name,p[:5], p.shape)\n",
    "        print(torch.allclose(p, phf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cdce50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speecht5.encoder.prenet.masked_spec_embed': 'speech_encoder_prenet.mask_emb',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.0.2.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias': 'speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.1.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.2.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.3.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.4.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.5.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight': 'speech_encoder_prenet.feature_extractor.conv_layers.6.0.weight',\n",
       " 'speecht5.encoder.prenet.feature_projection.layer_norm.weight': 'speech_encoder_prenet.layer_norm.weight',\n",
       " 'speecht5.encoder.prenet.feature_projection.layer_norm.bias': 'speech_encoder_prenet.layer_norm.bias',\n",
       " 'speecht5.encoder.prenet.feature_projection.projection.weight': 'speech_encoder_prenet.post_extract_proj.weight',\n",
       " 'speecht5.encoder.prenet.feature_projection.projection.bias': 'speech_encoder_prenet.post_extract_proj.bias',\n",
       " 'speecht5.encoder.prenet.pos_conv_embed.conv.bias': 'speech_encoder_prenet.pos_conv.0.bias',\n",
       " 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g': 'speech_encoder_prenet.pos_conv.0.weight_g',\n",
       " 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v': 'speech_encoder_prenet.pos_conv.0.weight_v'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prenet_mapping = {}\n",
    "\n",
    "hf_roi = 2\n",
    "bs_roi = 0\n",
    "for name, p in model_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.prenet\"):\n",
    "        #print(name, p.shape)\n",
    "        splitt = name.split('.')\n",
    "        for name_base, p_base in ckpt['model'].items():\n",
    "            if name_base.startswith(\"speech_encoder_prenet\"):\n",
    "                splitt_base = name_base.split('.')\n",
    "                \n",
    "                if splitt[hf_roi+1] == \"masked_spec_embed\" and splitt_base[bs_roi+1] == \"mask_emb\":\n",
    "                    prenet_mapping[name] = name_base\n",
    "                    break\n",
    "                elif \"feature_projection.layer_norm.weight\" in name and \"layer_norm.weight\" in name_base:\n",
    "                    prenet_mapping[name] = name_base\n",
    "                    break\n",
    "                elif \"feature_projection.layer_norm.bias\" in name and \"layer_norm.bias\" in name_base:\n",
    "                    prenet_mapping[name] = name_base\n",
    "                    break\n",
    "                elif \"feature_projection.projection.weight\" in name and \"post_extract_proj.weight\" in name_base:\n",
    "                    prenet_mapping[name] = name_base\n",
    "                    break\n",
    "                elif \"feature_projection.projection.bias\" in name and \"post_extract_proj.bias\" in name_base:\n",
    "                    prenet_mapping[name] = name_base\n",
    "                    break\n",
    "                elif \"feature_encoder\" in name and \"feature_extractor\" in name_base:\n",
    "                    #print(\"Original\", name_base)\n",
    "                    encoder_name = \"feature_encoder\"\n",
    "                    conv = \"conv\"\n",
    "                    layer_norm = \"layer_norm\"\n",
    "                    new_name_base = name_base.replace(\"feature_extractor\", encoder_name).replace(\"0.weight\",\"conv.weight\").replace(\"2.weight\", \"layer_norm.weight\").replace(\"2.bias\",\"layer_norm.bias\")\n",
    "                    #print(\"New:\", new_name_base)\n",
    "                    #print(f\"Splits {name.split('.',maxsplit=hf_roi+2)[-1]} =?= {new_name_base.split('.',maxsplit=hf_roi)[-1]}\")\n",
    "                    if name.split(\".\",maxsplit=hf_roi+2)[-1] == new_name_base.split(\".\",maxsplit=hf_roi)[-1]:\n",
    "                        prenet_mapping[name] = name_base\n",
    "                        break\n",
    "                elif \"pos_conv\" in name and \"pos_conv\" in name_base:\n",
    "                    new_name_base = name_base.replace(\"pos_conv\", \"pos_conv_embed\").replace(\"0\", \"conv\")\n",
    "                    #print(f\"Splits {name.split('.',maxsplit=hf_roi+1)[-1]} =?= {new_name_base.split('.',maxsplit=hf_roi-1)[-1]}\")\n",
    "                    if name.split(\".\",maxsplit=hf_roi+1)[-1] == new_name_base.split(\".\",maxsplit=hf_roi-1)[-1]:\n",
    "                        #print(\"HERE\")\n",
    "                        prenet_mapping[name] = name_base\n",
    "                        break \n",
    "                    \n",
    "                    \n",
    "prenet_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfab9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prenet_dict = {}\n",
    "for hf_layer, bs_layer in prenet_mapping.items():\n",
    "    for name, p in ckpt['model'].items():\n",
    "        if bs_layer == name:\n",
    "            prenet_dict[hf_layer] = p\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeac346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prenet_dict_state = {}\n",
    "for layer, param in prenet_dict.items():\n",
    "    new_layer = layer.split('.',maxsplit=3)[-1]\n",
    "    #print(new_layer)\n",
    "    prenet_dict_state[new_layer] = param\n",
    "    \n",
    "for name, param in model_asr.named_parameters():\n",
    "    if \"pos_sinusoidal_embed.weights\" in name:\n",
    "        prenet_dict_state[name.split('.',maxsplit=3)[-1]] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f549fc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_asr.speecht5.encoder.prenet.load_state_dict(prenet_dict_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c1a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for modified,original in zip(model_asr.speecht5.encoder.prenet.named_parameters(), model_orig.speecht5.encoder.prenet.named_parameters()):\n",
    "    print(torch.allclose(modified[1],original[1],rtol=1e-04, atol=1e-06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87fa1b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9727, 0.7222, 0.8418, 0.4727, 1.1113], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prenet_dict_state['feature_projection.layer_norm.weight'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7100bf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9727, 0.7222, 0.8418, 0.4727, 1.1113], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_asr.speecht5.encoder.prenet.feature_projection.layer_norm.weight[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccebf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9146, 0.6802, 0.7935, 0.4436, 1.0410], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig.speecht5.encoder.prenet.feature_projection.layer_norm.weight[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f96a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35eeaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb43d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6dae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11d137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ce940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dea33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85580c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeaf22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5433dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bbd563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bbbcbbb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"aaabaaa\".replace(\"b\",\"c\").replace(\"a\",\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "layer_type = splitt[hf_roi+1]\n",
    "                    layer_layer = splitt[hf_roi+2]\n",
    "                    layer_num = splitt[hf_roi+3]\n",
    "                    layer_block = splitt[hf_roi+4]\n",
    "                    layer_value = splitt[hf_roi+5]\n",
    "                    \n",
    "                    layer_type_base = splitt[bs_roi+1]\n",
    "                    layer_layer_base = splitt[bs_roi+2]\n",
    "                    layer_num_base = splitt[bs_roi+3]\n",
    "                    layer_block_base = splitt[bs_roi+4]\n",
    "                    layer_value_base = splitt[bs_roi+5]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eaec2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34164c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32106d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bca725",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = \"../extracted/speecht5/mapping/encoder_state_dict.pickle\"\n",
    "with open(mapping_file, 'rb') as handle:\n",
    "    encoder_state_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf30c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82eb3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d6a25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb.speecht5.encoder.wrapped_encoder.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1f5e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.ones((2,2)), torch.ones(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5724e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(model.speecht5.encoder.prenet.named_parameters(), modelb.speecht5.encoder.wrapped_encoder.named_parameters()):\n",
    "    print(torch.allclose(a[1],b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae5394d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5Encoder(\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x SpeechT5EncoderLayer(\n",
       "      (attention): SpeechT5Attention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): SpeechT5FeedForward(\n",
       "        (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "        (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "    (pe_k): Embedding(320, 64)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.speecht5.encoder.wrapped_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd004fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
