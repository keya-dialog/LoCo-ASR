{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e67a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from map_speecht5_hf import Mapping\n",
    "import torch\n",
    "from transformers import SpeechT5ForSpeechToText, SpeechT5ForTextToSpeech\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2991b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13216c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_asr = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\").to(device)\n",
    "model_tts = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc544b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"../checkpoints/speecht5_base.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfcf8afc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.version torch.Size([1])\n",
      "encoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.0.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.0.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.0.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.0.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.0.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.0.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.0.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.0.fc1.bias torch.Size([3072])\n",
      "encoder.layers.0.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.0.fc2.bias torch.Size([768])\n",
      "encoder.layers.0.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.0.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.0.norm_k.weight torch.Size([64])\n",
      "encoder.layers.0.norm_k.bias torch.Size([64])\n",
      "encoder.layers.1.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.1.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.1.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.1.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.1.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.1.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.1.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.1.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.1.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.1.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.1.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.1.fc1.bias torch.Size([3072])\n",
      "encoder.layers.1.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.1.fc2.bias torch.Size([768])\n",
      "encoder.layers.1.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.1.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.1.norm_k.weight torch.Size([64])\n",
      "encoder.layers.1.norm_k.bias torch.Size([64])\n",
      "encoder.layers.2.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.2.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.2.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.2.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.2.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.2.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.2.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.2.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.2.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.2.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.2.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.2.fc1.bias torch.Size([3072])\n",
      "encoder.layers.2.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.2.fc2.bias torch.Size([768])\n",
      "encoder.layers.2.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.2.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.2.norm_k.weight torch.Size([64])\n",
      "encoder.layers.2.norm_k.bias torch.Size([64])\n",
      "encoder.layers.3.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.3.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.3.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.3.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.3.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.3.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.3.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.3.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.3.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.3.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.3.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.3.fc1.bias torch.Size([3072])\n",
      "encoder.layers.3.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.3.fc2.bias torch.Size([768])\n",
      "encoder.layers.3.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.3.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.3.norm_k.weight torch.Size([64])\n",
      "encoder.layers.3.norm_k.bias torch.Size([64])\n",
      "encoder.layers.4.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.4.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.4.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.4.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.4.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.4.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.4.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.4.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.4.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.4.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.4.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.4.fc1.bias torch.Size([3072])\n",
      "encoder.layers.4.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.4.fc2.bias torch.Size([768])\n",
      "encoder.layers.4.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.4.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.4.norm_k.weight torch.Size([64])\n",
      "encoder.layers.4.norm_k.bias torch.Size([64])\n",
      "encoder.layers.5.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.5.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.5.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.5.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.5.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.5.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.5.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.5.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.5.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.5.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.5.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.5.fc1.bias torch.Size([3072])\n",
      "encoder.layers.5.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.5.fc2.bias torch.Size([768])\n",
      "encoder.layers.5.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.5.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.5.norm_k.weight torch.Size([64])\n",
      "encoder.layers.5.norm_k.bias torch.Size([64])\n",
      "encoder.layers.6.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.6.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.6.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.6.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.6.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.6.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.6.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.6.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.6.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.6.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.6.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.6.fc1.bias torch.Size([3072])\n",
      "encoder.layers.6.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.6.fc2.bias torch.Size([768])\n",
      "encoder.layers.6.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.6.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.6.norm_k.weight torch.Size([64])\n",
      "encoder.layers.6.norm_k.bias torch.Size([64])\n",
      "encoder.layers.7.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.7.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.7.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.7.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.7.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.7.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.7.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.7.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.7.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.7.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.7.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.7.fc1.bias torch.Size([3072])\n",
      "encoder.layers.7.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.7.fc2.bias torch.Size([768])\n",
      "encoder.layers.7.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.7.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.7.norm_k.weight torch.Size([64])\n",
      "encoder.layers.7.norm_k.bias torch.Size([64])\n",
      "encoder.layers.8.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.8.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.8.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.8.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.8.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.8.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.8.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.8.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.8.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.8.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.8.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.8.fc1.bias torch.Size([3072])\n",
      "encoder.layers.8.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.8.fc2.bias torch.Size([768])\n",
      "encoder.layers.8.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.8.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.8.norm_k.weight torch.Size([64])\n",
      "encoder.layers.8.norm_k.bias torch.Size([64])\n",
      "encoder.layers.9.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.9.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.9.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.9.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.9.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.9.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.9.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.9.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.9.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.9.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.9.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.9.fc1.bias torch.Size([3072])\n",
      "encoder.layers.9.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.9.fc2.bias torch.Size([768])\n",
      "encoder.layers.9.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.9.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.9.norm_k.weight torch.Size([64])\n",
      "encoder.layers.9.norm_k.bias torch.Size([64])\n",
      "encoder.layers.10.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.10.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.10.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.10.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.10.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.10.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.10.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.10.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.10.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.10.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.10.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.10.fc1.bias torch.Size([3072])\n",
      "encoder.layers.10.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.10.fc2.bias torch.Size([768])\n",
      "encoder.layers.10.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.10.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.10.norm_k.weight torch.Size([64])\n",
      "encoder.layers.10.norm_k.bias torch.Size([64])\n",
      "encoder.layers.11.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.11.self_attn.k_proj.bias torch.Size([768])\n",
      "encoder.layers.11.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.11.self_attn.v_proj.bias torch.Size([768])\n",
      "encoder.layers.11.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.11.self_attn.q_proj.bias torch.Size([768])\n",
      "encoder.layers.11.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "encoder.layers.11.self_attn.out_proj.bias torch.Size([768])\n",
      "encoder.layers.11.self_attn_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.11.self_attn_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.11.fc1.weight torch.Size([3072, 768])\n",
      "encoder.layers.11.fc1.bias torch.Size([3072])\n",
      "encoder.layers.11.fc2.weight torch.Size([768, 3072])\n",
      "encoder.layers.11.fc2.bias torch.Size([768])\n",
      "encoder.layers.11.final_layer_norm.weight torch.Size([768])\n",
      "encoder.layers.11.final_layer_norm.bias torch.Size([768])\n",
      "encoder.layers.11.norm_k.weight torch.Size([64])\n",
      "encoder.layers.11.norm_k.bias torch.Size([64])\n",
      "encoder.layer_norm.weight torch.Size([768])\n",
      "encoder.layer_norm.bias torch.Size([768])\n",
      "encoder.proj.weight torch.Size([81, 768])\n",
      "encoder.proj.bias torch.Size([81])\n",
      "encoder.pos_emb.pe_k.weight torch.Size([320, 64])\n",
      "decoder.version torch.Size([1])\n",
      "decoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.0.self_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.0.self_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.0.encoder_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.encoder_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.0.encoder_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.encoder_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.0.encoder_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.encoder_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.0.encoder_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.0.encoder_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.0.encoder_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.0.encoder_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.0.fc1.weight torch.Size([3072, 768])\n",
      "decoder.layers.0.fc1.bias torch.Size([3072])\n",
      "decoder.layers.0.fc2.weight torch.Size([768, 3072])\n",
      "decoder.layers.0.fc2.bias torch.Size([768])\n",
      "decoder.layers.0.final_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.0.final_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.0.norm_k.weight torch.Size([64])\n",
      "decoder.layers.0.norm_k.bias torch.Size([64])\n",
      "decoder.layers.1.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.1.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.1.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.1.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.1.self_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.1.self_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.1.encoder_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.encoder_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.1.encoder_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.encoder_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.1.encoder_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.encoder_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.1.encoder_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.1.encoder_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.1.encoder_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.1.encoder_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.1.fc1.weight torch.Size([3072, 768])\n",
      "decoder.layers.1.fc1.bias torch.Size([3072])\n",
      "decoder.layers.1.fc2.weight torch.Size([768, 3072])\n",
      "decoder.layers.1.fc2.bias torch.Size([768])\n",
      "decoder.layers.1.final_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.1.final_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.1.norm_k.weight torch.Size([64])\n",
      "decoder.layers.1.norm_k.bias torch.Size([64])\n",
      "decoder.layers.2.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.2.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.2.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.2.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.2.self_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.2.self_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.2.encoder_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.encoder_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.2.encoder_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.encoder_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.2.encoder_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.encoder_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.2.encoder_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.2.encoder_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.2.encoder_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.2.encoder_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.2.fc1.weight torch.Size([3072, 768])\n",
      "decoder.layers.2.fc1.bias torch.Size([3072])\n",
      "decoder.layers.2.fc2.weight torch.Size([768, 3072])\n",
      "decoder.layers.2.fc2.bias torch.Size([768])\n",
      "decoder.layers.2.final_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.2.final_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.2.norm_k.weight torch.Size([64])\n",
      "decoder.layers.2.norm_k.bias torch.Size([64])\n",
      "decoder.layers.3.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.3.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.3.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.3.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.3.self_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.3.self_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.3.encoder_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.encoder_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.3.encoder_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.encoder_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.3.encoder_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.encoder_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.3.encoder_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.3.encoder_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.3.encoder_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.3.encoder_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.3.fc1.weight torch.Size([3072, 768])\n",
      "decoder.layers.3.fc1.bias torch.Size([3072])\n",
      "decoder.layers.3.fc2.weight torch.Size([768, 3072])\n",
      "decoder.layers.3.fc2.bias torch.Size([768])\n",
      "decoder.layers.3.final_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.3.final_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.3.norm_k.weight torch.Size([64])\n",
      "decoder.layers.3.norm_k.bias torch.Size([64])\n",
      "decoder.layers.4.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.4.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.4.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.4.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.4.self_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.4.self_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.4.encoder_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.encoder_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.4.encoder_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.encoder_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.4.encoder_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.encoder_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.4.encoder_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.4.encoder_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.4.encoder_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.4.encoder_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.4.fc1.weight torch.Size([3072, 768])\n",
      "decoder.layers.4.fc1.bias torch.Size([3072])\n",
      "decoder.layers.4.fc2.weight torch.Size([768, 3072])\n",
      "decoder.layers.4.fc2.bias torch.Size([768])\n",
      "decoder.layers.4.final_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.4.final_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.4.norm_k.weight torch.Size([64])\n",
      "decoder.layers.4.norm_k.bias torch.Size([64])\n",
      "decoder.layers.5.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.5.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.5.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.5.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.5.self_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.5.self_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.5.encoder_attn.k_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.encoder_attn.k_proj.bias torch.Size([768])\n",
      "decoder.layers.5.encoder_attn.v_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.encoder_attn.v_proj.bias torch.Size([768])\n",
      "decoder.layers.5.encoder_attn.q_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.encoder_attn.q_proj.bias torch.Size([768])\n",
      "decoder.layers.5.encoder_attn.out_proj.weight torch.Size([768, 768])\n",
      "decoder.layers.5.encoder_attn.out_proj.bias torch.Size([768])\n",
      "decoder.layers.5.encoder_attn_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.5.encoder_attn_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.5.fc1.weight torch.Size([3072, 768])\n",
      "decoder.layers.5.fc1.bias torch.Size([3072])\n",
      "decoder.layers.5.fc2.weight torch.Size([768, 3072])\n",
      "decoder.layers.5.fc2.bias torch.Size([768])\n",
      "decoder.layers.5.final_layer_norm.weight torch.Size([768])\n",
      "decoder.layers.5.final_layer_norm.bias torch.Size([768])\n",
      "decoder.layers.5.norm_k.weight torch.Size([64])\n",
      "decoder.layers.5.norm_k.bias torch.Size([64])\n",
      "decoder.pos_emb.pe_k.weight torch.Size([320, 64])\n",
      "text_encoder_prenet.encoder_prenet.0.weight torch.Size([81, 768])\n",
      "text_encoder_prenet.encoder_prenet.1.alpha torch.Size([])\n",
      "speech_encoder_prenet.mask_emb torch.Size([768])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight torch.Size([512, 1, 10])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.0.2.weight torch.Size([512])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias torch.Size([512])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.1.0.weight torch.Size([512, 512, 3])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.2.0.weight torch.Size([512, 512, 3])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.3.0.weight torch.Size([512, 512, 3])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.4.0.weight torch.Size([512, 512, 3])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.5.0.weight torch.Size([512, 512, 2])\n",
      "speech_encoder_prenet.feature_extractor.conv_layers.6.0.weight torch.Size([512, 512, 2])\n",
      "speech_encoder_prenet.post_extract_proj.weight torch.Size([768, 512])\n",
      "speech_encoder_prenet.post_extract_proj.bias torch.Size([768])\n",
      "speech_encoder_prenet.layer_norm.weight torch.Size([512])\n",
      "speech_encoder_prenet.layer_norm.bias torch.Size([512])\n",
      "speech_encoder_prenet.pos_conv.0.bias torch.Size([768])\n",
      "speech_encoder_prenet.pos_conv.0.weight_g torch.Size([1, 1, 128])\n",
      "speech_encoder_prenet.pos_conv.0.weight_v torch.Size([768, 48, 128])\n",
      "speech_encoder_prenet.embed_positions._float_tensor torch.Size([1])\n",
      "text_decoder_prenet.embed_tokens.weight torch.Size([81, 768])\n",
      "text_decoder_prenet.embed_positions._float_tensor torch.Size([1])\n",
      "speech_decoder_prenet.decoder_prenet.0.0.prenet.0.0.weight torch.Size([256, 80])\n",
      "speech_decoder_prenet.decoder_prenet.0.0.prenet.0.0.bias torch.Size([256])\n",
      "speech_decoder_prenet.decoder_prenet.0.0.prenet.1.0.weight torch.Size([256, 256])\n",
      "speech_decoder_prenet.decoder_prenet.0.0.prenet.1.0.bias torch.Size([256])\n",
      "speech_decoder_prenet.decoder_prenet.0.1.weight torch.Size([768, 256])\n",
      "speech_decoder_prenet.decoder_prenet.0.1.bias torch.Size([768])\n",
      "speech_decoder_prenet.decoder_prenet.1.alpha torch.Size([])\n",
      "speech_decoder_prenet.spkembs_layer.0.weight torch.Size([768, 1280])\n",
      "speech_decoder_prenet.spkembs_layer.0.bias torch.Size([768])\n",
      "text_decoder_postnet.output_projection.weight torch.Size([81, 768])\n",
      "speech_decoder_postnet.feat_out.weight torch.Size([160, 768])\n",
      "speech_decoder_postnet.feat_out.bias torch.Size([160])\n",
      "speech_decoder_postnet.prob_out.weight torch.Size([2, 768])\n",
      "speech_decoder_postnet.prob_out.bias torch.Size([2])\n",
      "speech_decoder_postnet.postnet.postnet.0.0.weight torch.Size([256, 80, 5])\n",
      "speech_decoder_postnet.postnet.postnet.0.1.weight torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.0.1.bias torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.0.1.running_mean torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.0.1.running_var torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.0.1.num_batches_tracked torch.Size([])\n",
      "speech_decoder_postnet.postnet.postnet.1.0.weight torch.Size([256, 256, 5])\n",
      "speech_decoder_postnet.postnet.postnet.1.1.weight torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.1.1.bias torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.1.1.running_mean torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.1.1.running_var torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.1.1.num_batches_tracked torch.Size([])\n",
      "speech_decoder_postnet.postnet.postnet.2.0.weight torch.Size([256, 256, 5])\n",
      "speech_decoder_postnet.postnet.postnet.2.1.weight torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.2.1.bias torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.2.1.running_mean torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.2.1.running_var torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.2.1.num_batches_tracked torch.Size([])\n",
      "speech_decoder_postnet.postnet.postnet.3.0.weight torch.Size([256, 256, 5])\n",
      "speech_decoder_postnet.postnet.postnet.3.1.weight torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.3.1.bias torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.3.1.running_mean torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.3.1.running_var torch.Size([256])\n",
      "speech_decoder_postnet.postnet.postnet.3.1.num_batches_tracked torch.Size([])\n",
      "speech_decoder_postnet.postnet.postnet.4.0.weight torch.Size([80, 256, 5])\n",
      "speech_decoder_postnet.postnet.postnet.4.1.weight torch.Size([80])\n",
      "speech_decoder_postnet.postnet.postnet.4.1.bias torch.Size([80])\n",
      "speech_decoder_postnet.postnet.postnet.4.1.running_mean torch.Size([80])\n",
      "speech_decoder_postnet.postnet.postnet.4.1.running_var torch.Size([80])\n",
      "speech_decoder_postnet.postnet.postnet.4.1.num_batches_tracked torch.Size([])\n",
      "hubert_layer.label_embs_concat torch.Size([504, 256])\n",
      "hubert_layer.final_proj.weight torch.Size([256, 768])\n",
      "hubert_layer.final_proj.bias torch.Size([256])\n",
      "quantizer.vars torch.Size([1, 200, 384])\n",
      "quantizer.weight_proj.weight torch.Size([200, 768])\n",
      "quantizer.weight_proj.bias torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "for a,b in ckpt['model'].items():\n",
    "    print(a, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afbbb3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mapping_class = Mapping(model_asr, ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da647a66",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cdf47d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_state_dict = mapping_class.encoder_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a477224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_asr.speecht5.encoder.wrapped_encoder.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb8eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_folder = \"../extracted/speecht5/mapping/\"\n",
    "if not os.path.exists(mapping_folder):\n",
    "    os.makedirs(embeddings_folder)\n",
    "\n",
    "with open(os.path.join(mapping_folder, 'encoder_state_dict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(encoder_state_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfb555",
   "metadata": {},
   "source": [
    "## Speech Pre-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_prenet_state_dict = mapping_class.speech_prenet_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce81e96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_asr.speecht5.encoder.prenet.load_state_dict(speech_prenet_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2a751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_folder = \"../extracted/speecht5/mapping/\"\n",
    "if not os.path.exists(mapping_folder):\n",
    "    os.makedirs(embeddings_folder)\n",
    "\n",
    "with open(os.path.join(mapping_folder, 'speech_prenet_state_dict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(speech_prenet_state_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd288a13",
   "metadata": {},
   "source": [
    "## Text Pre-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c5f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prenet_state_dict = mapping_class.text_prenet_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a621e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_tokens.weight': tensor([[ 0.2236,  0.1462,  0.2703,  ..., -0.5044, -0.3403, -0.2323],\n",
       "         [-0.0267, -0.0211,  0.0806,  ...,  0.0826, -0.0250,  0.0142],\n",
       "         [ 0.1836,  0.1970,  0.1066,  ..., -0.0246,  0.0242, -0.1271],\n",
       "         ...,\n",
       "         [-0.0473, -0.0490,  0.0584,  ...,  0.1119, -0.1022, -0.0686],\n",
       "         [-0.0393, -0.1295,  0.0012,  ...,  0.0482, -0.0222, -0.3157],\n",
       "         [-0.0368, -0.0320,  0.0710,  ...,  0.1121, -0.0775, -0.0605]],\n",
       "        device='cuda:0'),\n",
       " 'encode_positions.alpha': tensor(0.1270, device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prenet_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a2465a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight\n",
      "encode_positions.alpha\n"
     ]
    }
   ],
   "source": [
    "for a,b in model_tts.speecht5.encoder.prenet.named_parameters():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b36b115d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ScaledPositionalEncoding(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tts.speecht5.encoder.prenet.encode_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12159294",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SpeechT5TextEncoderPrenet:\n\tMissing key(s) in state_dict: \"encode_positions.pe\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_tts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeecht5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprenet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_prenet_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SpeechT5TextEncoderPrenet:\n\tMissing key(s) in state_dict: \"encode_positions.pe\". "
     ]
    }
   ],
   "source": [
    "model_tts.speecht5.encoder.prenet.load_state_dict(text_prenet_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_folder = \"../extracted/speecht5/mapping/\"\n",
    "if not os.path.exists(mapping_folder):\n",
    "    os.makedirs(embeddings_folder)\n",
    "\n",
    "with open(os.path.join(mapping_folder, 'text_prenet_state_dict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(speech_prenet_state_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
