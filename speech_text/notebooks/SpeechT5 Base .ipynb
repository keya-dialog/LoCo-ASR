{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ba23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import SpeechT5ForSpeechToText#, SpeechT5Model, SpeechT5Config, SpeechT5PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652c4b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b15325",
   "metadata": {},
   "outputs": [],
   "source": [
    "st5_asr = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f7cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"../checkpoints/speecht5_base.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7216b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt['cfg']['task'].t5_task = 'pretrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "057b6d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ckpt['model'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943c12d",
   "metadata": {},
   "source": [
    "## Mapping Speech Pre-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8605ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_projection.layer_norm.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_projection.layer_norm.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.feature_projection.projection.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_projection.projection.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.pos_conv_embed.conv.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.pos_conv_embed.conv.weight_g -> speech_encoder_prenet.pos_conv.0.weight_g\n",
      "speecht5.encoder.prenet.pos_conv_embed.conv.weight_v -> speech_encoder_prenet.pos_conv.0.weight_v\n"
     ]
    }
   ],
   "source": [
    "# Initialize the mappings dictionary\n",
    "speech_prenet_mapping = {}\n",
    "\n",
    "# Iterate through st5_asr named_parameters\n",
    "for name, _ in st5_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.prenet\"):\n",
    "        for ckpt_name, ckpt_param in ckpt['model'].items():\n",
    "            if ckpt_name.startswith(\"speech_encoder_prenet\"):\n",
    "                # Split the layer names based on '.' and '_'\n",
    "                st5_asr_parts = name.split('.')\n",
    "                ckpt_parts = ckpt_name.split('.')\n",
    "                st5_asr_parts[-1] = st5_asr_parts[-1].split('_')[-1]\n",
    "                ckpt_parts[-1] = ckpt_parts[-1].split('_')[-1]\n",
    "\n",
    "                # Check if the layer names match\n",
    "                if st5_asr_parts[-1] == ckpt_parts[-1]:\n",
    "                    speech_prenet_mapping[name] = ckpt_name\n",
    "                    break\n",
    "\n",
    "# Print the mapping\n",
    "for st5_asr_name, ckpt_name in speech_prenet_mapping.items():\n",
    "    print(f\"{st5_asr_name} -> {ckpt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ead73f",
   "metadata": {},
   "source": [
    "## Mapping Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "772f5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_positions': {'pe_k': {}}, 'layers': {'0': {}, '1': {}, '2': {}, '3': {}, '4': {}, '5': {}, '6': {}, '7': {}, '8': {}, '9': {}, '10': {}, '11': {}}, 'layer_norm': {'weight': {}, 'bias': {}}}\n",
      "{'embed_positions': {'pe_k': {'weight': {}}}, 'layers': {'0': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '1': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '2': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '3': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '4': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '5': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '6': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '7': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '8': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '9': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '10': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}, '11': {'attention': {}, 'layer_norm': {}, 'feed_forward': {}, 'final_layer_norm': {}}}, 'layer_norm': {'weight': {}, 'bias': {}}}\n"
     ]
    }
   ],
   "source": [
    "encoder_mapping = {}\n",
    "listt = []\n",
    "hf_roi = 3\n",
    "encoder_name = \"speecht5.encoder.wrapped_encoder\"\n",
    "\n",
    "for name, _ in st5_asr.named_parameters():\n",
    "    if name.startswith(encoder_name):\n",
    "        listt.append(name.split(\".\")[hf_roi])\n",
    "\n",
    "layers = set(listt)\n",
    "#print(layers)\n",
    "\n",
    "st5_hf_dict = {l:{} for l in layers}\n",
    "#print(st5_hf_dict)\n",
    "\n",
    "for layer_name in layers:\n",
    "    temp_dict = {}\n",
    "    for name, _ in st5_asr.named_parameters():    \n",
    "        if name.startswith(encoder_name+\".\"+layer_name):\n",
    "            temp_dict[name.split('.')[hf_roi+1]] = {}\n",
    "    st5_hf_dict[layer_name] = temp_dict\n",
    "\n",
    "print(st5_hf_dict)\n",
    "\n",
    "for main_layer in st5_hf_dict.keys():\n",
    "    for sublayer in st5_hf_dict[main_layer]:\n",
    "        temp_dict = {}\n",
    "        for name, _ in st5_asr.named_parameters():\n",
    "            if name.startswith(encoder_name+\".\"+main_layer+\".\"+sublayer):\n",
    "                if len(name.split('.')) <= hf_roi+2:\n",
    "                    continue\n",
    "                temp_dict[name.split('.')[hf_roi+2]] = {}\n",
    "        st5_hf_dict[main_layer][sublayer] = temp_dict\n",
    "        \n",
    "print(st5_hf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ff0018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'version']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '0', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '1', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '2', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '3', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '4', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '5', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '6', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '7', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '8', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '9', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '10', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'k_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'v_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'q_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'out_proj', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'self_attn_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'fc1', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'fc1', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'fc2', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'fc2', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'final_layer_norm', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'final_layer_norm', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'norm_k', 'weight']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layers', '11', 'norm_k', 'bias']\n",
      "HF: ['speecht5', 'encoder', 'wrapped_encoder', 'layer_norm', 'weight']\n",
      "BS: ['encoder', 'layer_norm', 'weight']\n",
      "Here\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m st5_bs_layer_num \u001b[38;5;241m==\u001b[39m st5_bs_layer_num:\n\u001b[0;32m---> 27\u001b[0m     st5_hf_layer_num_layer \u001b[38;5;241m=\u001b[39m \u001b[43mst5_asr_parts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhf_roi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m     st5_bs_layer_num_layer \u001b[38;5;241m=\u001b[39m ckpt_parts[bs_roi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m st5_bs_layer_num_layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "encoder_mapping = {}\n",
    "\n",
    "#HF after wrapped_encoder: {'embed_positions', 'layer_norm', 'layers'}\n",
    "\n",
    "# Iterate through st5_asr named_parameters\n",
    "layer_num = 0\n",
    "hf_roi = 3\n",
    "bs_roi = 1\n",
    "for name, _ in st5_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.wrapped_encoder\"):\n",
    "        st5_asr_parts = name.split('.')\n",
    "        st5_hf_layer = st5_asr_parts[hf_roi]\n",
    "        #listt.append(st5_asr_parts[3])\n",
    "        for ckpt_name, _ in ckpt['model'].items():\n",
    "            if ckpt_name.startswith(\"encoder\"):\n",
    "                ckpt_parts = ckpt_name.split('.')\n",
    "                print(\"HF:\", st5_asr_parts)\n",
    "                print(\"BS:\", ckpt_parts)\n",
    "                st5_bs_layer = ckpt_parts[bs_roi]\n",
    "                \n",
    "                if st5_hf_layer == st5_bs_layer:\n",
    "                    st5_hf_layer_num = st5_asr_parts[hf_roi+1]\n",
    "                    st5_bs_layer_num = ckpt_parts[bs_roi+1]\n",
    "                    print(\"Here\")\n",
    "                    \n",
    "                    if st5_bs_layer_num == st5_bs_layer_num:\n",
    "                        st5_hf_layer_num_layer = st5_asr_parts[hf_roi+2]\n",
    "                        st5_bs_layer_num_layer = ckpt_parts[bs_roi+2]\n",
    "                        if st5_bs_layer_num_layer == \"self_attn\":\n",
    "                            st5_bs_layer_num_layer = \"attention\"\n",
    "                        print(\"Here\")\n",
    "                        \n",
    "                        if st5_hf_layer_num_layer == st5_bs_layer_num_layer:\n",
    "                            st5_hf_layer_num_layer_proj = st5_asr_parts[hf_roi+3]\n",
    "                            st5_bs_layer_num_layer_proj = ckpt_parts[bs_roi+3]\n",
    "                            print(\"Here\")\n",
    "                            \n",
    "                            if st5_hf_layer_num_layer_proj == st5_bs_layer_num_layer_proj:\n",
    "                                st5_hf_layer_num_layer_proj_param = st5_asr_parts[hf_roi+4]\n",
    "                                st5_bs_layer_num_layer_proj_param = ckpt_parts[bs_roi+4]\n",
    "                                print(\"Here\")\n",
    "                                \n",
    "                                if st5_hf_layer_num_layer_proj_param == st5_bs_layer_num_layer_proj_param:\n",
    "                                    encoder_mapping[name] = ckpt_name\n",
    "                                    break\n",
    "                        \n",
    "                        \n",
    "\n",
    "# Print the mapping\n",
    "for st5_asr_name, ckpt_name in encoder_mapping.items():\n",
    "    print(f\"{st5_asr_name} -> {ckpt_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a8d042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bf5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84639ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e3488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570193df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15116161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed609f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9aa30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e69d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb23451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558cc44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61771693",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 speecht5.encoder.wrapped_encoder.layer_norm.weight torch.Size([768])\n",
      "2 speecht5.encoder.wrapped_encoder.layer_norm.bias torch.Size([768])\n",
      "3 speecht5.encoder.wrapped_encoder.layers.0.attention.k_proj.weight torch.Size([768, 768])\n",
      "4 speecht5.encoder.wrapped_encoder.layers.0.attention.k_proj.bias torch.Size([768])\n",
      "5 speecht5.encoder.wrapped_encoder.layers.0.attention.v_proj.weight torch.Size([768, 768])\n",
      "6 speecht5.encoder.wrapped_encoder.layers.0.attention.v_proj.bias torch.Size([768])\n",
      "7 speecht5.encoder.wrapped_encoder.layers.0.attention.q_proj.weight torch.Size([768, 768])\n",
      "8 speecht5.encoder.wrapped_encoder.layers.0.attention.q_proj.bias torch.Size([768])\n",
      "9 speecht5.encoder.wrapped_encoder.layers.0.attention.out_proj.weight torch.Size([768, 768])\n",
      "10 speecht5.encoder.wrapped_encoder.layers.0.attention.out_proj.bias torch.Size([768])\n",
      "11 speecht5.encoder.wrapped_encoder.layers.0.layer_norm.weight torch.Size([768])\n",
      "12 speecht5.encoder.wrapped_encoder.layers.0.layer_norm.bias torch.Size([768])\n",
      "13 speecht5.encoder.wrapped_encoder.layers.0.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "14 speecht5.encoder.wrapped_encoder.layers.0.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "15 speecht5.encoder.wrapped_encoder.layers.0.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "16 speecht5.encoder.wrapped_encoder.layers.0.feed_forward.output_dense.bias torch.Size([768])\n",
      "17 speecht5.encoder.wrapped_encoder.layers.0.final_layer_norm.weight torch.Size([768])\n",
      "18 speecht5.encoder.wrapped_encoder.layers.0.final_layer_norm.bias torch.Size([768])\n",
      "19 speecht5.encoder.wrapped_encoder.layers.1.attention.k_proj.weight torch.Size([768, 768])\n",
      "20 speecht5.encoder.wrapped_encoder.layers.1.attention.k_proj.bias torch.Size([768])\n",
      "21 speecht5.encoder.wrapped_encoder.layers.1.attention.v_proj.weight torch.Size([768, 768])\n",
      "22 speecht5.encoder.wrapped_encoder.layers.1.attention.v_proj.bias torch.Size([768])\n",
      "23 speecht5.encoder.wrapped_encoder.layers.1.attention.q_proj.weight torch.Size([768, 768])\n",
      "24 speecht5.encoder.wrapped_encoder.layers.1.attention.q_proj.bias torch.Size([768])\n",
      "25 speecht5.encoder.wrapped_encoder.layers.1.attention.out_proj.weight torch.Size([768, 768])\n",
      "26 speecht5.encoder.wrapped_encoder.layers.1.attention.out_proj.bias torch.Size([768])\n",
      "27 speecht5.encoder.wrapped_encoder.layers.1.layer_norm.weight torch.Size([768])\n",
      "28 speecht5.encoder.wrapped_encoder.layers.1.layer_norm.bias torch.Size([768])\n",
      "29 speecht5.encoder.wrapped_encoder.layers.1.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "30 speecht5.encoder.wrapped_encoder.layers.1.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "31 speecht5.encoder.wrapped_encoder.layers.1.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "32 speecht5.encoder.wrapped_encoder.layers.1.feed_forward.output_dense.bias torch.Size([768])\n",
      "33 speecht5.encoder.wrapped_encoder.layers.1.final_layer_norm.weight torch.Size([768])\n",
      "34 speecht5.encoder.wrapped_encoder.layers.1.final_layer_norm.bias torch.Size([768])\n",
      "35 speecht5.encoder.wrapped_encoder.layers.2.attention.k_proj.weight torch.Size([768, 768])\n",
      "36 speecht5.encoder.wrapped_encoder.layers.2.attention.k_proj.bias torch.Size([768])\n",
      "37 speecht5.encoder.wrapped_encoder.layers.2.attention.v_proj.weight torch.Size([768, 768])\n",
      "38 speecht5.encoder.wrapped_encoder.layers.2.attention.v_proj.bias torch.Size([768])\n",
      "39 speecht5.encoder.wrapped_encoder.layers.2.attention.q_proj.weight torch.Size([768, 768])\n",
      "40 speecht5.encoder.wrapped_encoder.layers.2.attention.q_proj.bias torch.Size([768])\n",
      "41 speecht5.encoder.wrapped_encoder.layers.2.attention.out_proj.weight torch.Size([768, 768])\n",
      "42 speecht5.encoder.wrapped_encoder.layers.2.attention.out_proj.bias torch.Size([768])\n",
      "43 speecht5.encoder.wrapped_encoder.layers.2.layer_norm.weight torch.Size([768])\n",
      "44 speecht5.encoder.wrapped_encoder.layers.2.layer_norm.bias torch.Size([768])\n",
      "45 speecht5.encoder.wrapped_encoder.layers.2.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "46 speecht5.encoder.wrapped_encoder.layers.2.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "47 speecht5.encoder.wrapped_encoder.layers.2.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "48 speecht5.encoder.wrapped_encoder.layers.2.feed_forward.output_dense.bias torch.Size([768])\n",
      "49 speecht5.encoder.wrapped_encoder.layers.2.final_layer_norm.weight torch.Size([768])\n",
      "50 speecht5.encoder.wrapped_encoder.layers.2.final_layer_norm.bias torch.Size([768])\n",
      "51 speecht5.encoder.wrapped_encoder.layers.3.attention.k_proj.weight torch.Size([768, 768])\n",
      "52 speecht5.encoder.wrapped_encoder.layers.3.attention.k_proj.bias torch.Size([768])\n",
      "53 speecht5.encoder.wrapped_encoder.layers.3.attention.v_proj.weight torch.Size([768, 768])\n",
      "54 speecht5.encoder.wrapped_encoder.layers.3.attention.v_proj.bias torch.Size([768])\n",
      "55 speecht5.encoder.wrapped_encoder.layers.3.attention.q_proj.weight torch.Size([768, 768])\n",
      "56 speecht5.encoder.wrapped_encoder.layers.3.attention.q_proj.bias torch.Size([768])\n",
      "57 speecht5.encoder.wrapped_encoder.layers.3.attention.out_proj.weight torch.Size([768, 768])\n",
      "58 speecht5.encoder.wrapped_encoder.layers.3.attention.out_proj.bias torch.Size([768])\n",
      "59 speecht5.encoder.wrapped_encoder.layers.3.layer_norm.weight torch.Size([768])\n",
      "60 speecht5.encoder.wrapped_encoder.layers.3.layer_norm.bias torch.Size([768])\n",
      "61 speecht5.encoder.wrapped_encoder.layers.3.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "62 speecht5.encoder.wrapped_encoder.layers.3.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "63 speecht5.encoder.wrapped_encoder.layers.3.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "64 speecht5.encoder.wrapped_encoder.layers.3.feed_forward.output_dense.bias torch.Size([768])\n",
      "65 speecht5.encoder.wrapped_encoder.layers.3.final_layer_norm.weight torch.Size([768])\n",
      "66 speecht5.encoder.wrapped_encoder.layers.3.final_layer_norm.bias torch.Size([768])\n",
      "67 speecht5.encoder.wrapped_encoder.layers.4.attention.k_proj.weight torch.Size([768, 768])\n",
      "68 speecht5.encoder.wrapped_encoder.layers.4.attention.k_proj.bias torch.Size([768])\n",
      "69 speecht5.encoder.wrapped_encoder.layers.4.attention.v_proj.weight torch.Size([768, 768])\n",
      "70 speecht5.encoder.wrapped_encoder.layers.4.attention.v_proj.bias torch.Size([768])\n",
      "71 speecht5.encoder.wrapped_encoder.layers.4.attention.q_proj.weight torch.Size([768, 768])\n",
      "72 speecht5.encoder.wrapped_encoder.layers.4.attention.q_proj.bias torch.Size([768])\n",
      "73 speecht5.encoder.wrapped_encoder.layers.4.attention.out_proj.weight torch.Size([768, 768])\n",
      "74 speecht5.encoder.wrapped_encoder.layers.4.attention.out_proj.bias torch.Size([768])\n",
      "75 speecht5.encoder.wrapped_encoder.layers.4.layer_norm.weight torch.Size([768])\n",
      "76 speecht5.encoder.wrapped_encoder.layers.4.layer_norm.bias torch.Size([768])\n",
      "77 speecht5.encoder.wrapped_encoder.layers.4.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "78 speecht5.encoder.wrapped_encoder.layers.4.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "79 speecht5.encoder.wrapped_encoder.layers.4.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "80 speecht5.encoder.wrapped_encoder.layers.4.feed_forward.output_dense.bias torch.Size([768])\n",
      "81 speecht5.encoder.wrapped_encoder.layers.4.final_layer_norm.weight torch.Size([768])\n",
      "82 speecht5.encoder.wrapped_encoder.layers.4.final_layer_norm.bias torch.Size([768])\n",
      "83 speecht5.encoder.wrapped_encoder.layers.5.attention.k_proj.weight torch.Size([768, 768])\n",
      "84 speecht5.encoder.wrapped_encoder.layers.5.attention.k_proj.bias torch.Size([768])\n",
      "85 speecht5.encoder.wrapped_encoder.layers.5.attention.v_proj.weight torch.Size([768, 768])\n",
      "86 speecht5.encoder.wrapped_encoder.layers.5.attention.v_proj.bias torch.Size([768])\n",
      "87 speecht5.encoder.wrapped_encoder.layers.5.attention.q_proj.weight torch.Size([768, 768])\n",
      "88 speecht5.encoder.wrapped_encoder.layers.5.attention.q_proj.bias torch.Size([768])\n",
      "89 speecht5.encoder.wrapped_encoder.layers.5.attention.out_proj.weight torch.Size([768, 768])\n",
      "90 speecht5.encoder.wrapped_encoder.layers.5.attention.out_proj.bias torch.Size([768])\n",
      "91 speecht5.encoder.wrapped_encoder.layers.5.layer_norm.weight torch.Size([768])\n",
      "92 speecht5.encoder.wrapped_encoder.layers.5.layer_norm.bias torch.Size([768])\n",
      "93 speecht5.encoder.wrapped_encoder.layers.5.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "94 speecht5.encoder.wrapped_encoder.layers.5.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "95 speecht5.encoder.wrapped_encoder.layers.5.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "96 speecht5.encoder.wrapped_encoder.layers.5.feed_forward.output_dense.bias torch.Size([768])\n",
      "97 speecht5.encoder.wrapped_encoder.layers.5.final_layer_norm.weight torch.Size([768])\n",
      "98 speecht5.encoder.wrapped_encoder.layers.5.final_layer_norm.bias torch.Size([768])\n",
      "99 speecht5.encoder.wrapped_encoder.layers.6.attention.k_proj.weight torch.Size([768, 768])\n",
      "100 speecht5.encoder.wrapped_encoder.layers.6.attention.k_proj.bias torch.Size([768])\n",
      "101 speecht5.encoder.wrapped_encoder.layers.6.attention.v_proj.weight torch.Size([768, 768])\n",
      "102 speecht5.encoder.wrapped_encoder.layers.6.attention.v_proj.bias torch.Size([768])\n",
      "103 speecht5.encoder.wrapped_encoder.layers.6.attention.q_proj.weight torch.Size([768, 768])\n",
      "104 speecht5.encoder.wrapped_encoder.layers.6.attention.q_proj.bias torch.Size([768])\n",
      "105 speecht5.encoder.wrapped_encoder.layers.6.attention.out_proj.weight torch.Size([768, 768])\n",
      "106 speecht5.encoder.wrapped_encoder.layers.6.attention.out_proj.bias torch.Size([768])\n",
      "107 speecht5.encoder.wrapped_encoder.layers.6.layer_norm.weight torch.Size([768])\n",
      "108 speecht5.encoder.wrapped_encoder.layers.6.layer_norm.bias torch.Size([768])\n",
      "109 speecht5.encoder.wrapped_encoder.layers.6.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "110 speecht5.encoder.wrapped_encoder.layers.6.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "111 speecht5.encoder.wrapped_encoder.layers.6.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "112 speecht5.encoder.wrapped_encoder.layers.6.feed_forward.output_dense.bias torch.Size([768])\n",
      "113 speecht5.encoder.wrapped_encoder.layers.6.final_layer_norm.weight torch.Size([768])\n",
      "114 speecht5.encoder.wrapped_encoder.layers.6.final_layer_norm.bias torch.Size([768])\n",
      "115 speecht5.encoder.wrapped_encoder.layers.7.attention.k_proj.weight torch.Size([768, 768])\n",
      "116 speecht5.encoder.wrapped_encoder.layers.7.attention.k_proj.bias torch.Size([768])\n",
      "117 speecht5.encoder.wrapped_encoder.layers.7.attention.v_proj.weight torch.Size([768, 768])\n",
      "118 speecht5.encoder.wrapped_encoder.layers.7.attention.v_proj.bias torch.Size([768])\n",
      "119 speecht5.encoder.wrapped_encoder.layers.7.attention.q_proj.weight torch.Size([768, 768])\n",
      "120 speecht5.encoder.wrapped_encoder.layers.7.attention.q_proj.bias torch.Size([768])\n",
      "121 speecht5.encoder.wrapped_encoder.layers.7.attention.out_proj.weight torch.Size([768, 768])\n",
      "122 speecht5.encoder.wrapped_encoder.layers.7.attention.out_proj.bias torch.Size([768])\n",
      "123 speecht5.encoder.wrapped_encoder.layers.7.layer_norm.weight torch.Size([768])\n",
      "124 speecht5.encoder.wrapped_encoder.layers.7.layer_norm.bias torch.Size([768])\n",
      "125 speecht5.encoder.wrapped_encoder.layers.7.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "126 speecht5.encoder.wrapped_encoder.layers.7.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "127 speecht5.encoder.wrapped_encoder.layers.7.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "128 speecht5.encoder.wrapped_encoder.layers.7.feed_forward.output_dense.bias torch.Size([768])\n",
      "129 speecht5.encoder.wrapped_encoder.layers.7.final_layer_norm.weight torch.Size([768])\n",
      "130 speecht5.encoder.wrapped_encoder.layers.7.final_layer_norm.bias torch.Size([768])\n",
      "131 speecht5.encoder.wrapped_encoder.layers.8.attention.k_proj.weight torch.Size([768, 768])\n",
      "132 speecht5.encoder.wrapped_encoder.layers.8.attention.k_proj.bias torch.Size([768])\n",
      "133 speecht5.encoder.wrapped_encoder.layers.8.attention.v_proj.weight torch.Size([768, 768])\n",
      "134 speecht5.encoder.wrapped_encoder.layers.8.attention.v_proj.bias torch.Size([768])\n",
      "135 speecht5.encoder.wrapped_encoder.layers.8.attention.q_proj.weight torch.Size([768, 768])\n",
      "136 speecht5.encoder.wrapped_encoder.layers.8.attention.q_proj.bias torch.Size([768])\n",
      "137 speecht5.encoder.wrapped_encoder.layers.8.attention.out_proj.weight torch.Size([768, 768])\n",
      "138 speecht5.encoder.wrapped_encoder.layers.8.attention.out_proj.bias torch.Size([768])\n",
      "139 speecht5.encoder.wrapped_encoder.layers.8.layer_norm.weight torch.Size([768])\n",
      "140 speecht5.encoder.wrapped_encoder.layers.8.layer_norm.bias torch.Size([768])\n",
      "141 speecht5.encoder.wrapped_encoder.layers.8.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "142 speecht5.encoder.wrapped_encoder.layers.8.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "143 speecht5.encoder.wrapped_encoder.layers.8.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "144 speecht5.encoder.wrapped_encoder.layers.8.feed_forward.output_dense.bias torch.Size([768])\n",
      "145 speecht5.encoder.wrapped_encoder.layers.8.final_layer_norm.weight torch.Size([768])\n",
      "146 speecht5.encoder.wrapped_encoder.layers.8.final_layer_norm.bias torch.Size([768])\n",
      "147 speecht5.encoder.wrapped_encoder.layers.9.attention.k_proj.weight torch.Size([768, 768])\n",
      "148 speecht5.encoder.wrapped_encoder.layers.9.attention.k_proj.bias torch.Size([768])\n",
      "149 speecht5.encoder.wrapped_encoder.layers.9.attention.v_proj.weight torch.Size([768, 768])\n",
      "150 speecht5.encoder.wrapped_encoder.layers.9.attention.v_proj.bias torch.Size([768])\n",
      "151 speecht5.encoder.wrapped_encoder.layers.9.attention.q_proj.weight torch.Size([768, 768])\n",
      "152 speecht5.encoder.wrapped_encoder.layers.9.attention.q_proj.bias torch.Size([768])\n",
      "153 speecht5.encoder.wrapped_encoder.layers.9.attention.out_proj.weight torch.Size([768, 768])\n",
      "154 speecht5.encoder.wrapped_encoder.layers.9.attention.out_proj.bias torch.Size([768])\n",
      "155 speecht5.encoder.wrapped_encoder.layers.9.layer_norm.weight torch.Size([768])\n",
      "156 speecht5.encoder.wrapped_encoder.layers.9.layer_norm.bias torch.Size([768])\n",
      "157 speecht5.encoder.wrapped_encoder.layers.9.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "158 speecht5.encoder.wrapped_encoder.layers.9.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "159 speecht5.encoder.wrapped_encoder.layers.9.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "160 speecht5.encoder.wrapped_encoder.layers.9.feed_forward.output_dense.bias torch.Size([768])\n",
      "161 speecht5.encoder.wrapped_encoder.layers.9.final_layer_norm.weight torch.Size([768])\n",
      "162 speecht5.encoder.wrapped_encoder.layers.9.final_layer_norm.bias torch.Size([768])\n",
      "163 speecht5.encoder.wrapped_encoder.layers.10.attention.k_proj.weight torch.Size([768, 768])\n",
      "164 speecht5.encoder.wrapped_encoder.layers.10.attention.k_proj.bias torch.Size([768])\n",
      "165 speecht5.encoder.wrapped_encoder.layers.10.attention.v_proj.weight torch.Size([768, 768])\n",
      "166 speecht5.encoder.wrapped_encoder.layers.10.attention.v_proj.bias torch.Size([768])\n",
      "167 speecht5.encoder.wrapped_encoder.layers.10.attention.q_proj.weight torch.Size([768, 768])\n",
      "168 speecht5.encoder.wrapped_encoder.layers.10.attention.q_proj.bias torch.Size([768])\n",
      "169 speecht5.encoder.wrapped_encoder.layers.10.attention.out_proj.weight torch.Size([768, 768])\n",
      "170 speecht5.encoder.wrapped_encoder.layers.10.attention.out_proj.bias torch.Size([768])\n",
      "171 speecht5.encoder.wrapped_encoder.layers.10.layer_norm.weight torch.Size([768])\n",
      "172 speecht5.encoder.wrapped_encoder.layers.10.layer_norm.bias torch.Size([768])\n",
      "173 speecht5.encoder.wrapped_encoder.layers.10.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "174 speecht5.encoder.wrapped_encoder.layers.10.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "175 speecht5.encoder.wrapped_encoder.layers.10.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "176 speecht5.encoder.wrapped_encoder.layers.10.feed_forward.output_dense.bias torch.Size([768])\n",
      "177 speecht5.encoder.wrapped_encoder.layers.10.final_layer_norm.weight torch.Size([768])\n",
      "178 speecht5.encoder.wrapped_encoder.layers.10.final_layer_norm.bias torch.Size([768])\n",
      "179 speecht5.encoder.wrapped_encoder.layers.11.attention.k_proj.weight torch.Size([768, 768])\n",
      "180 speecht5.encoder.wrapped_encoder.layers.11.attention.k_proj.bias torch.Size([768])\n",
      "181 speecht5.encoder.wrapped_encoder.layers.11.attention.v_proj.weight torch.Size([768, 768])\n",
      "182 speecht5.encoder.wrapped_encoder.layers.11.attention.v_proj.bias torch.Size([768])\n",
      "183 speecht5.encoder.wrapped_encoder.layers.11.attention.q_proj.weight torch.Size([768, 768])\n",
      "184 speecht5.encoder.wrapped_encoder.layers.11.attention.q_proj.bias torch.Size([768])\n",
      "185 speecht5.encoder.wrapped_encoder.layers.11.attention.out_proj.weight torch.Size([768, 768])\n",
      "186 speecht5.encoder.wrapped_encoder.layers.11.attention.out_proj.bias torch.Size([768])\n",
      "187 speecht5.encoder.wrapped_encoder.layers.11.layer_norm.weight torch.Size([768])\n",
      "188 speecht5.encoder.wrapped_encoder.layers.11.layer_norm.bias torch.Size([768])\n",
      "189 speecht5.encoder.wrapped_encoder.layers.11.feed_forward.intermediate_dense.weight torch.Size([3072, 768])\n",
      "190 speecht5.encoder.wrapped_encoder.layers.11.feed_forward.intermediate_dense.bias torch.Size([3072])\n",
      "191 speecht5.encoder.wrapped_encoder.layers.11.feed_forward.output_dense.weight torch.Size([768, 3072])\n",
      "192 speecht5.encoder.wrapped_encoder.layers.11.feed_forward.output_dense.bias torch.Size([768])\n",
      "193 speecht5.encoder.wrapped_encoder.layers.11.final_layer_norm.weight torch.Size([768])\n",
      "194 speecht5.encoder.wrapped_encoder.layers.11.final_layer_norm.bias torch.Size([768])\n",
      "195 speecht5.encoder.wrapped_encoder.embed_positions.pe_k.weight torch.Size([320, 64])\n"
     ]
    }
   ],
   "source": [
    "ctr1 = 1\n",
    "for name, p in st5_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.wrapped_encoder\"):\n",
    "        print(ctr1, name, p.size())\n",
    "        ctr1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed27b50b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 encoder.version torch.Size([1])\n",
      "2 encoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "3 encoder.layers.0.self_attn.k_proj.bias torch.Size([768])\n",
      "4 encoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "5 encoder.layers.0.self_attn.v_proj.bias torch.Size([768])\n",
      "6 encoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "7 encoder.layers.0.self_attn.q_proj.bias torch.Size([768])\n",
      "8 encoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "9 encoder.layers.0.self_attn.out_proj.bias torch.Size([768])\n",
      "10 encoder.layers.0.self_attn_layer_norm.weight torch.Size([768])\n",
      "11 encoder.layers.0.self_attn_layer_norm.bias torch.Size([768])\n",
      "12 encoder.layers.0.fc1.weight torch.Size([3072, 768])\n",
      "13 encoder.layers.0.fc1.bias torch.Size([3072])\n",
      "14 encoder.layers.0.fc2.weight torch.Size([768, 3072])\n",
      "15 encoder.layers.0.fc2.bias torch.Size([768])\n",
      "16 encoder.layers.0.final_layer_norm.weight torch.Size([768])\n",
      "17 encoder.layers.0.final_layer_norm.bias torch.Size([768])\n",
      "18 encoder.layers.0.norm_k.weight torch.Size([64])\n",
      "19 encoder.layers.0.norm_k.bias torch.Size([64])\n",
      "20 encoder.layers.1.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "21 encoder.layers.1.self_attn.k_proj.bias torch.Size([768])\n",
      "22 encoder.layers.1.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "23 encoder.layers.1.self_attn.v_proj.bias torch.Size([768])\n",
      "24 encoder.layers.1.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "25 encoder.layers.1.self_attn.q_proj.bias torch.Size([768])\n",
      "26 encoder.layers.1.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "27 encoder.layers.1.self_attn.out_proj.bias torch.Size([768])\n",
      "28 encoder.layers.1.self_attn_layer_norm.weight torch.Size([768])\n",
      "29 encoder.layers.1.self_attn_layer_norm.bias torch.Size([768])\n",
      "30 encoder.layers.1.fc1.weight torch.Size([3072, 768])\n",
      "31 encoder.layers.1.fc1.bias torch.Size([3072])\n",
      "32 encoder.layers.1.fc2.weight torch.Size([768, 3072])\n",
      "33 encoder.layers.1.fc2.bias torch.Size([768])\n",
      "34 encoder.layers.1.final_layer_norm.weight torch.Size([768])\n",
      "35 encoder.layers.1.final_layer_norm.bias torch.Size([768])\n",
      "36 encoder.layers.1.norm_k.weight torch.Size([64])\n",
      "37 encoder.layers.1.norm_k.bias torch.Size([64])\n",
      "38 encoder.layers.2.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "39 encoder.layers.2.self_attn.k_proj.bias torch.Size([768])\n",
      "40 encoder.layers.2.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "41 encoder.layers.2.self_attn.v_proj.bias torch.Size([768])\n",
      "42 encoder.layers.2.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "43 encoder.layers.2.self_attn.q_proj.bias torch.Size([768])\n",
      "44 encoder.layers.2.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "45 encoder.layers.2.self_attn.out_proj.bias torch.Size([768])\n",
      "46 encoder.layers.2.self_attn_layer_norm.weight torch.Size([768])\n",
      "47 encoder.layers.2.self_attn_layer_norm.bias torch.Size([768])\n",
      "48 encoder.layers.2.fc1.weight torch.Size([3072, 768])\n",
      "49 encoder.layers.2.fc1.bias torch.Size([3072])\n",
      "50 encoder.layers.2.fc2.weight torch.Size([768, 3072])\n",
      "51 encoder.layers.2.fc2.bias torch.Size([768])\n",
      "52 encoder.layers.2.final_layer_norm.weight torch.Size([768])\n",
      "53 encoder.layers.2.final_layer_norm.bias torch.Size([768])\n",
      "54 encoder.layers.2.norm_k.weight torch.Size([64])\n",
      "55 encoder.layers.2.norm_k.bias torch.Size([64])\n",
      "56 encoder.layers.3.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "57 encoder.layers.3.self_attn.k_proj.bias torch.Size([768])\n",
      "58 encoder.layers.3.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "59 encoder.layers.3.self_attn.v_proj.bias torch.Size([768])\n",
      "60 encoder.layers.3.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "61 encoder.layers.3.self_attn.q_proj.bias torch.Size([768])\n",
      "62 encoder.layers.3.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "63 encoder.layers.3.self_attn.out_proj.bias torch.Size([768])\n",
      "64 encoder.layers.3.self_attn_layer_norm.weight torch.Size([768])\n",
      "65 encoder.layers.3.self_attn_layer_norm.bias torch.Size([768])\n",
      "66 encoder.layers.3.fc1.weight torch.Size([3072, 768])\n",
      "67 encoder.layers.3.fc1.bias torch.Size([3072])\n",
      "68 encoder.layers.3.fc2.weight torch.Size([768, 3072])\n",
      "69 encoder.layers.3.fc2.bias torch.Size([768])\n",
      "70 encoder.layers.3.final_layer_norm.weight torch.Size([768])\n",
      "71 encoder.layers.3.final_layer_norm.bias torch.Size([768])\n",
      "72 encoder.layers.3.norm_k.weight torch.Size([64])\n",
      "73 encoder.layers.3.norm_k.bias torch.Size([64])\n",
      "74 encoder.layers.4.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "75 encoder.layers.4.self_attn.k_proj.bias torch.Size([768])\n",
      "76 encoder.layers.4.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "77 encoder.layers.4.self_attn.v_proj.bias torch.Size([768])\n",
      "78 encoder.layers.4.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "79 encoder.layers.4.self_attn.q_proj.bias torch.Size([768])\n",
      "80 encoder.layers.4.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "81 encoder.layers.4.self_attn.out_proj.bias torch.Size([768])\n",
      "82 encoder.layers.4.self_attn_layer_norm.weight torch.Size([768])\n",
      "83 encoder.layers.4.self_attn_layer_norm.bias torch.Size([768])\n",
      "84 encoder.layers.4.fc1.weight torch.Size([3072, 768])\n",
      "85 encoder.layers.4.fc1.bias torch.Size([3072])\n",
      "86 encoder.layers.4.fc2.weight torch.Size([768, 3072])\n",
      "87 encoder.layers.4.fc2.bias torch.Size([768])\n",
      "88 encoder.layers.4.final_layer_norm.weight torch.Size([768])\n",
      "89 encoder.layers.4.final_layer_norm.bias torch.Size([768])\n",
      "90 encoder.layers.4.norm_k.weight torch.Size([64])\n",
      "91 encoder.layers.4.norm_k.bias torch.Size([64])\n",
      "92 encoder.layers.5.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "93 encoder.layers.5.self_attn.k_proj.bias torch.Size([768])\n",
      "94 encoder.layers.5.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "95 encoder.layers.5.self_attn.v_proj.bias torch.Size([768])\n",
      "96 encoder.layers.5.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "97 encoder.layers.5.self_attn.q_proj.bias torch.Size([768])\n",
      "98 encoder.layers.5.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "99 encoder.layers.5.self_attn.out_proj.bias torch.Size([768])\n",
      "100 encoder.layers.5.self_attn_layer_norm.weight torch.Size([768])\n",
      "101 encoder.layers.5.self_attn_layer_norm.bias torch.Size([768])\n",
      "102 encoder.layers.5.fc1.weight torch.Size([3072, 768])\n",
      "103 encoder.layers.5.fc1.bias torch.Size([3072])\n",
      "104 encoder.layers.5.fc2.weight torch.Size([768, 3072])\n",
      "105 encoder.layers.5.fc2.bias torch.Size([768])\n",
      "106 encoder.layers.5.final_layer_norm.weight torch.Size([768])\n",
      "107 encoder.layers.5.final_layer_norm.bias torch.Size([768])\n",
      "108 encoder.layers.5.norm_k.weight torch.Size([64])\n",
      "109 encoder.layers.5.norm_k.bias torch.Size([64])\n",
      "110 encoder.layers.6.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "111 encoder.layers.6.self_attn.k_proj.bias torch.Size([768])\n",
      "112 encoder.layers.6.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "113 encoder.layers.6.self_attn.v_proj.bias torch.Size([768])\n",
      "114 encoder.layers.6.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "115 encoder.layers.6.self_attn.q_proj.bias torch.Size([768])\n",
      "116 encoder.layers.6.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "117 encoder.layers.6.self_attn.out_proj.bias torch.Size([768])\n",
      "118 encoder.layers.6.self_attn_layer_norm.weight torch.Size([768])\n",
      "119 encoder.layers.6.self_attn_layer_norm.bias torch.Size([768])\n",
      "120 encoder.layers.6.fc1.weight torch.Size([3072, 768])\n",
      "121 encoder.layers.6.fc1.bias torch.Size([3072])\n",
      "122 encoder.layers.6.fc2.weight torch.Size([768, 3072])\n",
      "123 encoder.layers.6.fc2.bias torch.Size([768])\n",
      "124 encoder.layers.6.final_layer_norm.weight torch.Size([768])\n",
      "125 encoder.layers.6.final_layer_norm.bias torch.Size([768])\n",
      "126 encoder.layers.6.norm_k.weight torch.Size([64])\n",
      "127 encoder.layers.6.norm_k.bias torch.Size([64])\n",
      "128 encoder.layers.7.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "129 encoder.layers.7.self_attn.k_proj.bias torch.Size([768])\n",
      "130 encoder.layers.7.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "131 encoder.layers.7.self_attn.v_proj.bias torch.Size([768])\n",
      "132 encoder.layers.7.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "133 encoder.layers.7.self_attn.q_proj.bias torch.Size([768])\n",
      "134 encoder.layers.7.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "135 encoder.layers.7.self_attn.out_proj.bias torch.Size([768])\n",
      "136 encoder.layers.7.self_attn_layer_norm.weight torch.Size([768])\n",
      "137 encoder.layers.7.self_attn_layer_norm.bias torch.Size([768])\n",
      "138 encoder.layers.7.fc1.weight torch.Size([3072, 768])\n",
      "139 encoder.layers.7.fc1.bias torch.Size([3072])\n",
      "140 encoder.layers.7.fc2.weight torch.Size([768, 3072])\n",
      "141 encoder.layers.7.fc2.bias torch.Size([768])\n",
      "142 encoder.layers.7.final_layer_norm.weight torch.Size([768])\n",
      "143 encoder.layers.7.final_layer_norm.bias torch.Size([768])\n",
      "144 encoder.layers.7.norm_k.weight torch.Size([64])\n",
      "145 encoder.layers.7.norm_k.bias torch.Size([64])\n",
      "146 encoder.layers.8.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "147 encoder.layers.8.self_attn.k_proj.bias torch.Size([768])\n",
      "148 encoder.layers.8.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "149 encoder.layers.8.self_attn.v_proj.bias torch.Size([768])\n",
      "150 encoder.layers.8.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "151 encoder.layers.8.self_attn.q_proj.bias torch.Size([768])\n",
      "152 encoder.layers.8.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "153 encoder.layers.8.self_attn.out_proj.bias torch.Size([768])\n",
      "154 encoder.layers.8.self_attn_layer_norm.weight torch.Size([768])\n",
      "155 encoder.layers.8.self_attn_layer_norm.bias torch.Size([768])\n",
      "156 encoder.layers.8.fc1.weight torch.Size([3072, 768])\n",
      "157 encoder.layers.8.fc1.bias torch.Size([3072])\n",
      "158 encoder.layers.8.fc2.weight torch.Size([768, 3072])\n",
      "159 encoder.layers.8.fc2.bias torch.Size([768])\n",
      "160 encoder.layers.8.final_layer_norm.weight torch.Size([768])\n",
      "161 encoder.layers.8.final_layer_norm.bias torch.Size([768])\n",
      "162 encoder.layers.8.norm_k.weight torch.Size([64])\n",
      "163 encoder.layers.8.norm_k.bias torch.Size([64])\n",
      "164 encoder.layers.9.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "165 encoder.layers.9.self_attn.k_proj.bias torch.Size([768])\n",
      "166 encoder.layers.9.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "167 encoder.layers.9.self_attn.v_proj.bias torch.Size([768])\n",
      "168 encoder.layers.9.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "169 encoder.layers.9.self_attn.q_proj.bias torch.Size([768])\n",
      "170 encoder.layers.9.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "171 encoder.layers.9.self_attn.out_proj.bias torch.Size([768])\n",
      "172 encoder.layers.9.self_attn_layer_norm.weight torch.Size([768])\n",
      "173 encoder.layers.9.self_attn_layer_norm.bias torch.Size([768])\n",
      "174 encoder.layers.9.fc1.weight torch.Size([3072, 768])\n",
      "175 encoder.layers.9.fc1.bias torch.Size([3072])\n",
      "176 encoder.layers.9.fc2.weight torch.Size([768, 3072])\n",
      "177 encoder.layers.9.fc2.bias torch.Size([768])\n",
      "178 encoder.layers.9.final_layer_norm.weight torch.Size([768])\n",
      "179 encoder.layers.9.final_layer_norm.bias torch.Size([768])\n",
      "180 encoder.layers.9.norm_k.weight torch.Size([64])\n",
      "181 encoder.layers.9.norm_k.bias torch.Size([64])\n",
      "182 encoder.layers.10.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "183 encoder.layers.10.self_attn.k_proj.bias torch.Size([768])\n",
      "184 encoder.layers.10.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "185 encoder.layers.10.self_attn.v_proj.bias torch.Size([768])\n",
      "186 encoder.layers.10.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "187 encoder.layers.10.self_attn.q_proj.bias torch.Size([768])\n",
      "188 encoder.layers.10.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "189 encoder.layers.10.self_attn.out_proj.bias torch.Size([768])\n",
      "190 encoder.layers.10.self_attn_layer_norm.weight torch.Size([768])\n",
      "191 encoder.layers.10.self_attn_layer_norm.bias torch.Size([768])\n",
      "192 encoder.layers.10.fc1.weight torch.Size([3072, 768])\n",
      "193 encoder.layers.10.fc1.bias torch.Size([3072])\n",
      "194 encoder.layers.10.fc2.weight torch.Size([768, 3072])\n",
      "195 encoder.layers.10.fc2.bias torch.Size([768])\n",
      "196 encoder.layers.10.final_layer_norm.weight torch.Size([768])\n",
      "197 encoder.layers.10.final_layer_norm.bias torch.Size([768])\n",
      "198 encoder.layers.10.norm_k.weight torch.Size([64])\n",
      "199 encoder.layers.10.norm_k.bias torch.Size([64])\n",
      "200 encoder.layers.11.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "201 encoder.layers.11.self_attn.k_proj.bias torch.Size([768])\n",
      "202 encoder.layers.11.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "203 encoder.layers.11.self_attn.v_proj.bias torch.Size([768])\n",
      "204 encoder.layers.11.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "205 encoder.layers.11.self_attn.q_proj.bias torch.Size([768])\n",
      "206 encoder.layers.11.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "207 encoder.layers.11.self_attn.out_proj.bias torch.Size([768])\n",
      "208 encoder.layers.11.self_attn_layer_norm.weight torch.Size([768])\n",
      "209 encoder.layers.11.self_attn_layer_norm.bias torch.Size([768])\n",
      "210 encoder.layers.11.fc1.weight torch.Size([3072, 768])\n",
      "211 encoder.layers.11.fc1.bias torch.Size([3072])\n",
      "212 encoder.layers.11.fc2.weight torch.Size([768, 3072])\n",
      "213 encoder.layers.11.fc2.bias torch.Size([768])\n",
      "214 encoder.layers.11.final_layer_norm.weight torch.Size([768])\n",
      "215 encoder.layers.11.final_layer_norm.bias torch.Size([768])\n",
      "216 encoder.layers.11.norm_k.weight torch.Size([64])\n",
      "217 encoder.layers.11.norm_k.bias torch.Size([64])\n",
      "218 encoder.layer_norm.weight torch.Size([768])\n",
      "219 encoder.layer_norm.bias torch.Size([768])\n",
      "220 encoder.proj.weight torch.Size([81, 768])\n",
      "221 encoder.proj.bias torch.Size([81])\n",
      "222 encoder.pos_emb.pe_k.weight torch.Size([320, 64])\n"
     ]
    }
   ],
   "source": [
    "compat = {}\n",
    "ctr2 = 1\n",
    "for name, p in ckpt['model'].items():\n",
    "    if name.startswith(\"encoder\"):\n",
    "        print(ctr2, name, p.size())\n",
    "        ctr2 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd44fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 speech_encoder_prenet.mask_emb -> speecht5.encoder.prenet.masked_spec_emb\n",
      "2 speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight\n",
      "3 speech_encoder_prenet.feature_extractor.conv_layers.0.2.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.0.2.weight\n",
      "4 speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias -> speecht5.encoder.prenet.feature_encoder.conv_layers.0.2.bias\n",
      "5 speech_encoder_prenet.feature_extractor.conv_layers.1.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight\n",
      "6 speech_encoder_prenet.feature_extractor.conv_layers.2.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight\n",
      "7 speech_encoder_prenet.feature_extractor.conv_layers.3.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight\n",
      "8 speech_encoder_prenet.feature_extractor.conv_layers.4.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight\n",
      "9 speech_encoder_prenet.feature_extractor.conv_layers.5.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight\n",
      "10 speech_encoder_prenet.feature_extractor.conv_layers.6.0.weight -> speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight\n",
      "11 speech_encoder_prenet.post_extract_proj.weight -> speecht5.encoder.prenet.post_extract_proj.weight\n",
      "12 speech_encoder_prenet.post_extract_proj.bias -> speecht5.encoder.prenet.post_extract_proj.bias\n",
      "13 speech_encoder_prenet.layer_norm.weight -> speecht5.encoder.prenet.layer_norm.weight\n",
      "14 speech_encoder_prenet.layer_norm.bias -> speecht5.encoder.prenet.layer_norm.bias\n",
      "15 speech_encoder_prenet.pos_conv.0.bias -> speecht5.encoder.prenet.pos_conv.0.bias\n",
      "16 speech_encoder_prenet.pos_conv.0.weight_g -> speecht5.encoder.prenet.pos_conv.conv.weight_g\n",
      "17 speech_encoder_prenet.pos_conv.0.weight_v -> speecht5.encoder.prenet.pos_conv.conv.weight_v\n",
      "18 speech_encoder_prenet.embed_positions._float_tensor -> speecht5.encoder.prenet.embed_positions._float_tensor\n"
     ]
    }
   ],
   "source": [
    "compat = {}\n",
    "ctr2 = 1\n",
    "for name, p in ckpt['model'].items():\n",
    "    if name.startswith(\"speech_encoder_prenet\"):\n",
    "        print(ctr2, name, end=\" -> \")\n",
    "        parts = name.split(\".\")\n",
    "        tmp = name.replace(\"speech\", \"speecht5\")\n",
    "        tmp = tmp.replace(\"_\", \".\", 2)\n",
    "        tmp = tmp.replace(\"feature_extractor\", \"feature_encoder\")\n",
    "        tmp = tmp.replace(\".0.weight\", \".conv.weight\")\n",
    "        \n",
    "        if tmp.endswith(\"mask_emb\"):\n",
    "            tmp = tmp.replace(\"mask_emb\", \"masked_spec_emb\")\n",
    "        \n",
    "        print(tmp)\n",
    "        ctr2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81db279",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "\n",
    "for name, p in ckpt['model'].items():\n",
    "    if name.startswith(\"speech_encoder_prenet\"):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new state dictionary using the mapped layer names\n",
    "new_state_dict = {}\n",
    "for name, param in ckpt['model'].items():\n",
    "    if name in compat:\n",
    "        new_name = compat[name]\n",
    "        new_state_dict[new_name] = param\n",
    "\n",
    "# Load the new state dictionary into the Hugging Face model\n",
    "st5_asr.model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Now you can use the Hugging Face model with some input text\n",
    "input_text = \"Your input text here\"\n",
    "output = st5_asr(input_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d37c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82286692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e6882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b852070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213412bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9dd4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a931eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928f641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c137431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1801cb1",
   "metadata": {},
   "source": [
    "## Mapping Speech Pre-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69e9c3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_projection.layer_norm.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_projection.layer_norm.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.feature_projection.projection.weight -> speech_encoder_prenet.feature_extractor.conv_layers.0.0.weight\n",
      "speecht5.encoder.prenet.feature_projection.projection.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.pos_conv_embed.conv.bias -> speech_encoder_prenet.feature_extractor.conv_layers.0.2.bias\n",
      "speecht5.encoder.prenet.pos_conv_embed.conv.weight_g -> speech_encoder_prenet.pos_conv.0.weight_g\n",
      "speecht5.encoder.prenet.pos_conv_embed.conv.weight_v -> speech_encoder_prenet.pos_conv.0.weight_v\n"
     ]
    }
   ],
   "source": [
    "# Initialize the mappings dictionary\n",
    "speech_prenet_mapping = {}\n",
    "\n",
    "# Iterate through st5_asr named_parameters\n",
    "for name, _ in st5_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.prenet\"):\n",
    "        for ckpt_name, ckpt_param in ckpt['model'].items():\n",
    "            if ckpt_name.startswith(\"speech_encoder_prenet\"):\n",
    "                # Split the layer names based on '.' and '_'\n",
    "                st5_asr_parts = name.split('.')\n",
    "                ckpt_parts = ckpt_name.split('.')\n",
    "                st5_asr_parts[-1] = st5_asr_parts[-1].split('_')[-1]\n",
    "                ckpt_parts[-1] = ckpt_parts[-1].split('_')[-1]\n",
    "\n",
    "                # Check if the layer names match\n",
    "                if st5_asr_parts[-1] == ckpt_parts[-1]:\n",
    "                    speech_prenet_mapping[name] = ckpt_name\n",
    "                    break\n",
    "\n",
    "# Print the mapping\n",
    "for st5_asr_name, ckpt_name in speech_prenet_mapping.items():\n",
    "    print(f\"{st5_asr_name} -> {ckpt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1b9db",
   "metadata": {},
   "source": [
    "## Mapping Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fb1d04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_positions', 'layer_norm', 'layers'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_mapping = {}\n",
    "\n",
    "listt = []\n",
    "#HF after wrapped_encoder: {'embed_positions', 'layer_norm', 'layers'}\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through st5_asr named_parameters\n",
    "layer_num = 0\n",
    "for name, _ in st5_asr.named_parameters():\n",
    "    if name.startswith(\"speecht5.encoder.wrapped_encoder\"):\n",
    "        st5_asr_parts = name.split('.')\n",
    "        listt.append(st5_asr_parts[3])\n",
    "        #for ckpt_name, _ in ckpt['model'].items():\n",
    "        #    if ckpt_name.startswith(\"encoder\"):\n",
    "        #        st5_asr_parts = name.split('.')\n",
    "        #        ckpt_parts = ckpt_name.split('.')\n",
    "        #        print(st5_asr_parts)\n",
    "\n",
    "sett = set(listt)\n",
    "sett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386b6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f91ffd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "577eed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speecht5.encoder.wrapped_encoder.layers.0.attention.k_proj.weight -> encoder.layers.0.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.1.attention.k_proj.weight -> encoder.layers.11.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.2.attention.k_proj.weight -> encoder.layers.2.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.3.attention.k_proj.weight -> encoder.layers.3.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.4.attention.k_proj.weight -> encoder.layers.4.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.5.attention.k_proj.weight -> encoder.layers.5.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.6.attention.k_proj.weight -> encoder.layers.6.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.7.attention.k_proj.weight -> encoder.layers.7.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.8.attention.k_proj.weight -> encoder.layers.8.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.9.attention.k_proj.weight -> encoder.layers.9.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.10.attention.k_proj.weight -> encoder.layers.10.norm_k.bias\n",
      "speecht5.encoder.wrapped_encoder.layers.11.attention.k_proj.weight -> encoder.layers.11.norm_k.bias\n"
     ]
    }
   ],
   "source": [
    "# Initialize the mappings dictionary\n",
    "mapping = {}\n",
    "\n",
    "# Iterate through ckpt['model'] items\n",
    "for ckpt_name, ckpt_param in ckpt['model'].items():\n",
    "    if ckpt_name.startswith(\"encoder.layers.\"):\n",
    "        # Get the layer number from the ckpt_name\n",
    "        ckpt_layer_num = int(ckpt_name.split(\".\")[2])\n",
    "\n",
    "        # Iterate through st5_asr named_parameters\n",
    "        for name, _ in st5_asr.named_parameters():\n",
    "            if name.startswith(\"speecht5.encoder.wrapped_encoder.layers.\"):\n",
    "                # Get the layer number from the st5_asr name\n",
    "                st5_asr_layer_num = int(name.split(\".\")[-2])\n",
    "\n",
    "                # Check if the layer numbers match\n",
    "                if ckpt_layer_num == st5_asr_layer_num:\n",
    "                    mapping[name] = ckpt_name\n",
    "                    break\n",
    "\n",
    "# Print the mapping\n",
    "for st5_asr_name, ckpt_name in mapping.items():\n",
    "    print(f\"{st5_asr_name} -> {ckpt_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "effae9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all layer numbers from 0 to 11 for both models\n",
    "for layer_num in range(12):\n",
    "    ckpt_layer_prefix = f\"encoder.layers.{layer_num}\"\n",
    "    st5_asr_layer_prefix = f\"speecht5.encoder.wrapped_encoder.layers.{layer_num}\"\n",
    "\n",
    "    # Iterate through ckpt['model'] items\n",
    "    for ckpt_name, ckpt_param in ckpt['model'].items():\n",
    "        if ckpt_name.startswith(ckpt_layer_prefix):\n",
    "            # Iterate through st5_asr named_parameters\n",
    "            for name, _ in st5_asr.named_parameters():\n",
    "                if name.startswith(st5_asr_layer_prefix):\n",
    "                    mapping[name] = ckpt_name\n",
    "                    break\n",
    "\n",
    "# Print the mapping\n",
    "for st5_asr_name, ckpt_name in mapping.items():\n",
    "    print(f\"{st5_asr_name} -> {ckpt_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
