{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec83f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForSpeechToText, SpeechT5Tokenizer, SpeechT5Model, SpeechT5FeatureExtractor\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11336a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50925a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_demo (/export/home/lium/bdos/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "example = dataset[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4c1c0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb992d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8103d2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5FeatureExtractor {\n",
       "  \"do_normalize\": false,\n",
       "  \"feature_extractor_type\": \"SpeechT5FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"fmax\": 7600,\n",
       "  \"fmin\": 80,\n",
       "  \"frame_signal_scale\": 1.0,\n",
       "  \"hop_length\": 16,\n",
       "  \"mel_floor\": 1e-10,\n",
       "  \"num_mel_bins\": 80,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"SpeechT5Processor\",\n",
       "  \"reduction_factor\": 2,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"win_function\": \"hann_window\",\n",
       "  \"win_length\": 64\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = SpeechT5FeatureExtractor.from_pretrained(\"microsoft/speecht5_asr\")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2478b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor.do_normalize = True\n",
    "#feature_extractor.feature_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5387d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor.do_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d2bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/speecht5_asr were not used when initializing SpeechT5Model: ['text_decoder_postnet.lm_head.weight', 'speecht5.decoder.prenet.embed_tokens.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.decoder.prenet.embed_positions.weights', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "stt = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\")\n",
    "stt = stt.to(device)\n",
    "model = SpeechT5Model.from_pretrained(\"microsoft/speecht5_asr\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59f9ffae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForSpeechToText(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1-4): 4 x SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5-6): 2 x SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (pos_sinusoidal_embed): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a470af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "#inputs = processor(audio=example[\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs2 = feature_extractor(example[\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sampling_rate, truncation=True, max_length=768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3080b9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df8f439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-4.8828e-04, -1.8311e-04, -1.3733e-03, -1.2207e-04, -7.3242e-04,\n",
       "          4.2725e-04,  1.2817e-03,  1.8616e-03,  3.0212e-03, -4.2725e-04,\n",
       "         -4.2725e-04,  9.1553e-04, -1.8311e-04, -1.5869e-03,  7.3242e-04,\n",
       "          4.1809e-03,  1.4343e-03, -6.7139e-04,  2.4414e-04, -4.5776e-04,\n",
       "         -1.1597e-03,  1.5259e-04,  2.5940e-03, -3.9673e-04, -2.2583e-03,\n",
       "          5.1880e-04,  8.8501e-04, -1.1292e-03, -1.6479e-03,  2.5635e-03,\n",
       "          2.4109e-03, -1.6785e-03, -2.1057e-03, -1.8311e-04,  8.5449e-04,\n",
       "          8.8501e-04,  3.7842e-03,  1.7395e-03, -8.8501e-04, -3.9673e-04,\n",
       "          3.0518e-04, -2.4414e-04, -9.1553e-05,  2.7466e-03,  2.1667e-03,\n",
       "         -9.1553e-05,  9.7656e-04,  1.1902e-03,  2.4414e-04,  6.4087e-04,\n",
       "          8.2397e-04, -1.3428e-03, -3.4180e-03, -1.6479e-03,  6.4087e-04,\n",
       "          6.4087e-04,  8.8501e-04,  2.2278e-03,  2.2278e-03,  6.1035e-05,\n",
       "         -1.4038e-03, -5.4932e-04, -1.1902e-03, -2.3193e-03, -6.7139e-04,\n",
       "          1.2207e-03,  1.8616e-03, -5.1880e-04,  1.9531e-03,  1.5259e-03,\n",
       "         -1.1292e-03, -2.4719e-03, -1.5869e-03,  1.6174e-03,  9.1553e-05,\n",
       "         -7.6294e-04, -8.5449e-04, -7.9346e-04, -6.1035e-04, -1.2512e-03,\n",
       "         -2.2278e-03, -9.7656e-04, -1.6785e-03, -1.7700e-03, -6.7139e-04,\n",
       "         -1.5564e-03, -5.7983e-04, -6.4087e-04, -5.1880e-04, -2.5635e-03,\n",
       "         -4.6692e-03, -1.4343e-03,  4.8828e-04, -2.1362e-04, -1.5564e-03,\n",
       "         -1.4343e-03,  1.6785e-03, -1.0681e-03, -2.1973e-03, -7.3242e-04,\n",
       "         -7.3242e-04, -1.2207e-03, -1.2817e-03, -5.4932e-04, -2.1973e-03,\n",
       "         -6.1035e-05,  2.1057e-03, -2.1362e-04, -3.5095e-03, -2.8076e-03,\n",
       "         -9.1553e-04, -1.1292e-03, -3.2959e-03, -1.6174e-03,  2.6245e-03,\n",
       "          1.2207e-03, -1.3123e-03, -1.1902e-03,  3.9673e-04, -7.3242e-04,\n",
       "         -6.7139e-04,  5.7983e-04, -2.9602e-03, -2.7771e-03,  1.1902e-03,\n",
       "          8.5449e-04,  4.2725e-04, -1.0681e-03,  6.1035e-04, -1.4343e-03,\n",
       "         -1.0986e-03, -2.8381e-03, -2.7466e-03,  2.7771e-03,  2.1362e-03,\n",
       "         -3.0518e-04, -3.4790e-03,  1.3733e-03, -2.4414e-04, -2.4414e-03,\n",
       "         -1.6174e-03, -1.4343e-03, -2.7466e-03, -2.3804e-03,  1.2512e-03,\n",
       "         -1.8311e-04, -4.8828e-04,  2.8687e-03,  1.8005e-03, -3.7842e-03,\n",
       "         -4.4250e-03, -1.4343e-03, -1.0376e-03, -1.7090e-03, -3.0518e-05,\n",
       "          3.0518e-05,  8.2397e-04,  8.8501e-04,  6.1035e-05, -8.8501e-04,\n",
       "         -7.9346e-04, -1.3733e-03, -3.9978e-03, -2.6855e-03, -1.9836e-03,\n",
       "          4.5776e-04,  2.5330e-03,  1.7395e-03,  1.5259e-04, -1.2512e-03,\n",
       "          5.7983e-04, -1.5259e-03, -2.7161e-03, -2.3499e-03,  3.6621e-04,\n",
       "          2.8381e-03,  1.2817e-03, -1.8311e-04,  5.4932e-04,  1.8616e-03,\n",
       "         -3.6621e-04, -1.3428e-03, -1.8616e-03, -1.8311e-04, -9.1553e-05,\n",
       "         -1.2207e-03, -7.0190e-04, -1.0681e-03,  1.5564e-03,  6.7139e-04,\n",
       "          3.0518e-04, -4.8828e-04, -3.1128e-03, -6.4087e-04,  1.8005e-03,\n",
       "         -1.5259e-04, -8.2397e-04,  1.2207e-03,  2.4414e-03, -1.3733e-03,\n",
       "         -2.4414e-03, -2.1362e-04,  3.6621e-04, -2.4414e-04,  1.0986e-03,\n",
       "          9.1553e-04, -1.4648e-03,  1.0071e-03,  1.1902e-03,  9.1553e-05,\n",
       "         -3.2349e-03, -1.5259e-04,  1.7700e-03, -7.0190e-04, -2.7466e-04,\n",
       "          2.7466e-04,  1.0376e-03, -5.4932e-04,  3.0518e-05, -9.1553e-04,\n",
       "         -2.4719e-03, -1.8921e-03, -9.1553e-05, -3.6621e-04, -1.3123e-03,\n",
       "         -2.1362e-04,  1.8311e-03,  8.8501e-04, -2.2583e-03, -1.7700e-03,\n",
       "         -9.7656e-04, -4.5776e-04,  2.4414e-04,  8.2397e-04,  3.9673e-04,\n",
       "          9.1553e-04, -4.2725e-04, -9.4604e-04, -1.5259e-03,  1.5259e-03,\n",
       "          1.4954e-03,  3.0518e-04,  3.9673e-04, -6.1035e-04, -3.0518e-04,\n",
       "          1.2512e-03,  6.1035e-04, -9.4604e-04, -6.1035e-04,  7.0190e-04,\n",
       "          1.0376e-03, -2.1057e-03,  8.5449e-04,  2.0447e-03,  5.7983e-04,\n",
       "         -2.7466e-04,  1.2512e-03,  2.4719e-03,  9.1553e-04,  1.5259e-04,\n",
       "          7.0190e-04, -5.4932e-04,  1.2207e-04,  2.9602e-03,  8.5449e-04,\n",
       "          1.8921e-03,  6.7139e-04,  1.3123e-03, -7.6294e-04,  6.1035e-05,\n",
       "          2.4414e-03,  6.7139e-04,  2.0752e-03,  6.4087e-04, -6.1035e-04,\n",
       "         -2.4414e-04,  6.7139e-04,  9.7656e-04, -4.5776e-04, -4.8828e-04,\n",
       "          2.7466e-04,  7.9346e-04,  3.6011e-03,  3.2349e-03,  1.3428e-03,\n",
       "          3.6621e-04, -7.3242e-04, -7.0190e-04, -2.5024e-03, -7.6294e-04,\n",
       "          2.1362e-03,  2.3499e-03,  2.3193e-03,  2.5635e-03,  1.4343e-03,\n",
       "         -5.4932e-04, -1.1292e-03,  3.3569e-04, -2.6245e-03, -2.6245e-03,\n",
       "          1.3428e-03,  2.8687e-03,  1.8921e-03,  1.5259e-03,  2.7466e-03,\n",
       "          8.8501e-04, -1.7090e-03, -1.8005e-03, -2.1362e-03, -1.0681e-03,\n",
       "          8.2397e-04,  1.3733e-03,  2.9602e-03,  1.8311e-03,  1.5259e-03,\n",
       "         -1.2207e-04, -2.4414e-04, -1.5259e-04, -8.5449e-04,  1.8311e-04,\n",
       "         -7.9346e-04,  4.5776e-04,  1.0986e-03,  1.1292e-03,  1.3733e-03,\n",
       "          1.2207e-03,  1.5259e-04, -2.4719e-03, -3.1738e-03, -7.6294e-04,\n",
       "         -1.8311e-04,  1.5869e-03,  8.2397e-04,  6.1035e-04,  1.0071e-03,\n",
       "          1.1902e-03, -5.7983e-04, -1.5869e-03, -2.7466e-04,  0.0000e+00,\n",
       "         -1.1292e-03, -6.1035e-04,  1.4954e-03,  2.4719e-03,  1.5564e-03,\n",
       "         -1.2207e-04, -1.0681e-03, -2.7771e-03, -1.5259e-03, -1.5259e-04,\n",
       "          6.1035e-05, -3.6621e-04,  9.1553e-05,  2.3499e-03, -1.5259e-04,\n",
       "         -3.2959e-03, -1.4648e-03,  8.5449e-04,  3.6621e-04, -2.3193e-03,\n",
       "         -7.9346e-04, -2.7466e-04, -1.2207e-04, -2.4414e-04, -3.3569e-04,\n",
       "         -3.0518e-04,  5.7983e-04,  5.4932e-04, -1.6479e-03, -1.8616e-03,\n",
       "         -4.5776e-04,  9.4604e-04,  9.4604e-04, -1.2207e-04, -5.7983e-04,\n",
       "          2.1362e-04,  1.5259e-04, -1.8311e-03, -1.8311e-03,  3.0518e-05,\n",
       "          2.1362e-04,  0.0000e+00, -3.6621e-04, -1.8311e-04, -5.1880e-04,\n",
       "         -1.5259e-04, -1.7700e-03, -3.0518e-03, -7.0190e-04,  3.3569e-04,\n",
       "          2.7466e-04, -2.4414e-04,  1.3428e-03,  2.4109e-03,  4.2725e-04,\n",
       "         -1.1597e-03, -2.0447e-03, -1.8311e-04,  5.7983e-04,  4.5776e-04,\n",
       "          2.5940e-03,  2.3193e-03,  1.4648e-03,  8.2397e-04,  1.1597e-03,\n",
       "         -2.1362e-04, -2.2888e-03,  2.7466e-04,  7.0190e-04, -1.5259e-03,\n",
       "          9.1553e-04,  3.1433e-03,  1.9226e-03, -9.1553e-04,  1.2207e-04,\n",
       "          7.9346e-04, -2.9297e-03, -3.6621e-04,  1.7090e-03,  9.7656e-04,\n",
       "          2.1667e-03,  3.2043e-03,  3.2654e-03, -8.8501e-04, -2.2888e-03,\n",
       "          6.4087e-04, -4.8828e-04, -1.5564e-03, -2.8381e-03, -4.2725e-04,\n",
       "          1.0986e-03, -1.3123e-03,  2.4109e-03,  2.3804e-03, -1.0681e-03,\n",
       "         -1.8616e-03, -2.0752e-03, -3.4180e-03, -3.9368e-03,  3.9673e-04,\n",
       "          3.2043e-03,  3.3569e-04,  1.4038e-03,  2.1973e-03,  1.5259e-03,\n",
       "         -4.8828e-04, -2.8381e-03, -1.5869e-03, -1.8921e-03, -6.4087e-04,\n",
       "          2.4414e-04,  1.5869e-03,  3.4485e-03,  1.4954e-03,  1.4343e-03,\n",
       "          5.7983e-04, -2.7771e-03, -2.9602e-03, -2.4719e-03, -9.1553e-05,\n",
       "         -3.9673e-04, -3.3569e-04,  3.0212e-03,  1.8005e-03, -9.4604e-04,\n",
       "         -6.7139e-04, -3.0518e-04,  2.7466e-04, -1.5259e-04,  4.8828e-04,\n",
       "         -6.1035e-05, -2.5635e-03,  6.7139e-04,  2.3193e-03,  4.8828e-04,\n",
       "          3.6621e-04,  1.8311e-03,  3.6621e-03,  4.8828e-04, -2.1362e-03,\n",
       "          7.6294e-04,  2.7161e-03, -6.1035e-05, -3.6926e-03, -1.8616e-03,\n",
       "          1.7090e-03,  9.4604e-04,  2.0142e-03,  3.7842e-03,  1.8311e-03,\n",
       "          8.5449e-04,  1.8311e-04, -1.9531e-03, -4.0588e-03, -3.5400e-03,\n",
       "          4.2725e-04,  7.3242e-04,  9.1553e-05,  2.8992e-03,  7.2937e-03,\n",
       "          5.8899e-03, -5.4932e-04, -1.6174e-03, -1.8616e-03, -5.2185e-03,\n",
       "         -4.6692e-03,  1.7090e-03,  3.6316e-03,  2.3193e-03,  4.9133e-03,\n",
       "          5.9204e-03,  2.0752e-03, -1.7090e-03, -1.0681e-03, -3.0518e-05,\n",
       "         -1.9226e-03, -2.2278e-03,  6.1035e-04,  4.2419e-03,  3.8147e-03,\n",
       "          2.3804e-03,  4.0894e-03,  1.8921e-03, -5.4932e-04, -1.6785e-03,\n",
       "         -1.2512e-03, -9.1553e-04, -1.0986e-03,  3.1433e-03,  2.9602e-03,\n",
       "          1.5259e-03, -7.9346e-04,  5.1880e-04,  2.4414e-04, -4.5166e-03,\n",
       "         -4.3945e-03,  2.5940e-03,  5.1270e-03,  1.1597e-03,  9.7656e-04,\n",
       "          1.7090e-03, -2.7466e-04, -2.0142e-03,  6.1035e-04, -1.2817e-03,\n",
       "         -1.5869e-03,  3.0518e-04,  1.3733e-03, -7.9346e-04, -3.6621e-04,\n",
       "          3.9673e-03,  2.5940e-03, -8.8501e-04, -3.9673e-03, -3.2349e-03,\n",
       "         -1.4038e-03, -6.1035e-04,  9.4604e-04,  8.8501e-04,  1.7700e-03,\n",
       "          2.9297e-03,  1.2207e-03, -1.4954e-03, -3.6011e-03,  3.0518e-05,\n",
       "          5.7983e-04, -2.1362e-03, -8.5449e-04,  2.3193e-03,  3.9978e-03,\n",
       "          2.6245e-03,  7.3242e-04, -2.4414e-03, -2.7466e-03, -6.1035e-04,\n",
       "         -6.1035e-04, -3.2959e-03,  1.8311e-04,  4.5776e-03,  2.8381e-03,\n",
       "         -1.5869e-03, -2.1057e-03,  4.8828e-04, -1.3733e-03, -3.8757e-03,\n",
       "         -2.9907e-03, -7.3242e-04,  7.9346e-04,  2.1057e-03,  2.5024e-03,\n",
       "          1.8616e-03, -6.1035e-04,  1.4343e-03,  4.2725e-04, -4.4861e-03,\n",
       "         -4.7607e-03,  1.4038e-03,  4.6692e-03, -1.2817e-03, -1.1597e-03,\n",
       "          4.1199e-03,  2.7161e-03, -2.8381e-03, -1.1902e-03,  8.5449e-04,\n",
       "         -2.6550e-03, -3.7537e-03, -9.1553e-05,  5.1880e-04, -1.8921e-03,\n",
       "          2.1057e-03,  4.4861e-03, -2.1362e-04, -2.1973e-03,  5.1880e-04,\n",
       "         -6.1035e-05, -4.2725e-03, -2.0142e-03,  3.7537e-03,  3.3569e-04,\n",
       "         -1.8921e-03,  7.3242e-04,  2.6855e-03, -7.6294e-04, -1.5869e-03,\n",
       "          3.2959e-03,  8.8501e-04, -4.8828e-04, -1.1597e-03, -1.0986e-03,\n",
       "         -3.5400e-03, -2.9297e-03, -3.0518e-05,  7.3242e-04,  6.7139e-04,\n",
       "          1.7395e-03,  1.0986e-03, -6.1035e-04, -1.0376e-03, -1.0681e-03,\n",
       "         -1.3123e-03, -2.7771e-03, -6.7139e-04,  2.4414e-04,  5.7983e-04,\n",
       "          6.7139e-04,  6.4087e-04,  7.6294e-04,  1.0681e-03,  1.8005e-03,\n",
       "         -1.0376e-03, -3.0212e-03, -1.6174e-03, -1.2207e-04, -2.3193e-03,\n",
       "         -2.4109e-03, -6.1035e-05,  2.0752e-03,  3.0518e-05, -2.7466e-03,\n",
       "         -1.2207e-03, -9.4604e-04, -4.8828e-04, -2.6855e-03, -1.6785e-03,\n",
       "         -1.0071e-03, -1.9836e-03, -6.1035e-05,  4.2725e-04, -1.0986e-03,\n",
       "         -1.8921e-03,  1.0376e-03,  1.3733e-03, -2.6245e-03, -2.4414e-03,\n",
       "          1.3733e-03, -1.0376e-03, -2.8687e-03, -2.8687e-03, -7.0190e-04,\n",
       "          0.0000e+00, -1.5259e-04,  1.9531e-03, -6.1035e-05, -1.6479e-03,\n",
       "         -5.1880e-04, -3.0518e-04, -2.2888e-03, -2.9907e-03, -4.2725e-04,\n",
       "          1.7090e-03, -2.3804e-03, -1.4343e-03,  1.7090e-03,  1.9226e-03,\n",
       "          3.0518e-05, -3.8757e-03, -1.4343e-03, -5.4932e-04,  5.1880e-04,\n",
       "          4.2725e-04, -5.7983e-04,  2.0142e-03,  1.7700e-03,  3.6621e-04,\n",
       "         -2.1057e-03, -1.5259e-03,  1.7700e-03,  1.9226e-03, -1.1292e-03,\n",
       "         -1.3428e-03,  3.3569e-04,  2.7161e-03,  1.6785e-03, -1.8921e-03,\n",
       "         -7.3242e-04, -3.9673e-04,  9.1553e-05, -2.2583e-03, -3.1433e-03,\n",
       "          1.5869e-03,  4.2725e-03,  1.6785e-03, -1.1902e-03, -1.0681e-03,\n",
       "          2.0142e-03,  1.0071e-03, -2.4109e-03, -1.8311e-04,  1.8616e-03,\n",
       "          3.2654e-03,  9.4604e-04, -1.6479e-03,  5.4932e-04,  1.2512e-03,\n",
       "          9.4604e-04, -7.9346e-04, -9.7656e-04,  2.1057e-03,  1.6785e-03,\n",
       "          9.7656e-04, -3.0518e-04, -7.3242e-04,  2.0142e-03,  7.9346e-04,\n",
       "         -2.3193e-03, -4.2419e-03, -3.6621e-04,  2.8381e-03,  7.3242e-04,\n",
       "          5.4932e-04,  2.8992e-03,  2.8687e-03,  3.0518e-04, -2.1057e-03,\n",
       "         -6.7139e-04,  1.5869e-03, -2.4414e-04, -9.4604e-04, -1.6785e-03,\n",
       "         -4.8828e-04, -2.0447e-03, -2.7466e-04,  1.9226e-03,  3.9673e-04,\n",
       "          1.2207e-03,  1.9226e-03,  5.7983e-04]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0', dtype=torch.int32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e694b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.input_values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e9580f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2.input_values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63e716d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpeechT5Tokenizer.from_pretrained(\"microsoft/speecht5_asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaa9d1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5Tokenizer(name_or_path='microsoft/speecht5_asr', vocab_size=79, model_max_length=450, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e0fcc",
   "metadata": {},
   "source": [
    "## Speech Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec4bea1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify `decoder_input_ids`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the embeddings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mstt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/transformers/models/speecht5/modeling_speecht5.py:2468\u001b[0m, in \u001b[0;36mSpeechT5ForSpeechToText.forward\u001b[0;34m(self, input_values, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2464\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2465\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2466\u001b[0m         )\n\u001b[0;32m-> 2468\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeecht5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2484\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_decoder_postnet(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2486\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/transformers/models/speecht5/modeling_speecht5.py:2292\u001b[0m, in \u001b[0;36mSpeechT5Model.forward\u001b[0;34m(self, input_values, attention_mask, decoder_input_values, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, use_cache, speaker_embeddings, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2290\u001b[0m     decoder_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 2292\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2302\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2304\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoder_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   2308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/transformers/models/speecht5/modeling_speecht5.py:1864\u001b[0m, in \u001b[0;36mSpeechT5DecoderWithTextPrenet.forward\u001b[0;34m(self, input_values, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1852\u001b[0m     input_values: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1863\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n\u001b[0;32m-> 1864\u001b[0m     decoder_hidden_states, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1866\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped_decoder(\n\u001b[1;32m   1867\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mdecoder_hidden_states,\n\u001b[1;32m   1868\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1877\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1878\u001b[0m     )\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transf/lib/python3.9/site-packages/transformers/models/speecht5/modeling_speecht5.py:863\u001b[0m, in \u001b[0;36mSpeechT5TextDecoderPrenet.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values)\u001b[0m\n\u001b[1;32m    861\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify `decoder_input_ids`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    865\u001b[0m past_key_values_length \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    866\u001b[0m positions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions(input_ids, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify `decoder_input_ids`"
     ]
    }
   ],
   "source": [
    "# Compute the embeddings\n",
    "with torch.no_grad():\n",
    "    out = stt.forward(inputs2.input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the embeddings\n",
    "#with torch.no_grad():\n",
    "#    embeddings = model(**inputs).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71196221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
