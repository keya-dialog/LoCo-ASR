#!/bin/bash
#PBS -q qgpu
#PBS -A OPEN-28-58
#PBS -l select=1:ncpus=16:ngpus=1
#PBS -l walltime=0:30:00
#PBS -N seq2seq_fisher

source /apps/all/Anaconda3/2022.05/bin/activate ~/.conda/envs/loco_asr/

SRC_DIR="/mnt/proj1/open-28-58/lakoc/LoCo-ASR/joinning_enc_dec"
SCRATCH_DIR="/scratch/project/open-28-58/lakoc"
DATASET_DIR="${SCRATCH_DIR}/fisher"
VAL_IDS="${SCRATCH_DIR}/val_ids"
EXPERIMENT="XLS-R300m+cold_decoder_test3"

cd $SRC_DIR
export HF_HOME="${SRC_DIR}/huggingface_cache"

WANDB_RUN_ID=$EXPERIMENT WANDB_PROJECT="LoCo-ASR" \
  python src/trainers/xlsr+gpt2.py \
  --dataset_name="${DATASET_DIR}" \
  --max_duration_in_seconds="20.0" \
  --min_duration_in_seconds="2.0" \
  --train_split="train_500" \
  --base_encoder_model="facebook/wav2vec2-xls-r-300m" \
  --feature_extractor_name="facebook/wav2vec2-xls-r-300m" \
  --base_decoder_model="gpt2" \
  --tokenizer_name="gpt2" \
  --enc_layers_to_freeze="24" \
  --steps_to_freeze_enc="-1" \
  --output_dir="${SCRATCH_DIR}/${EXPERIMENT}" \
  --gradient_accumulation_steps="2" \
  --learning_rate="1e-4" \
  --logging_steps="10" \
  --save_strategy="steps" \
  --save_steps="1000" \
  --evaluation_strategy="steps" \
  --eval_steps="1000" \
  --per_device_train_batch_size="64" \
  --per_device_eval_batch_size="64" \
  --report_to="wandb" \
  --optim="adamw_torch" \
  --dataloader_num_workers="4" \
  --group_by_length="True" \
  --length_column_name="input_len" \
  --load_best_model_at_end="True" \
  --metric_for_best_model="eval_wer" \
  --early_stopping_patience="5" \
  --remove_unused_columns="False" \
  --predict_with_generate="True" \
  --save_total_limit="5" \
  --num_train_epochs=10 \
  --decoder_cold_start="True" \
  --num_beams="5" \
  --greater_is_better="False" \
  --val_indexes_to_use="${VAL_IDS}"
