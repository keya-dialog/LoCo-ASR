#!/bin/bash
#PBS -q qgpu
#PBS -A OPEN-28-58
#PBS -l select=1:ncpus=16:ngpus=1
#PBS -l walltime=24:00:00
#PBS -N seq2seq_fisher

source /apps/all/Anaconda3/2022.05/bin/activate ~/.conda/envs/ec_asr/

SRC_DIR=/mnt/proj1/open-28-58/lakoc/LoCo-ASR/joinning_enc_dec
cd $SRC_DIR

export HF_HOME="${SRC_DIR}/huggingface_cache"
EXPERIMENT="XLS-R300m+warm_decoder+decoder_different_lr"

WANDB_RUN_ID=$EXPERIMENT WANDB_PROJECT="LoCo-ASR" \
  python src/trainers/xlsr+gpt2.py \
  --dataset_name="${SRC_DIR}/datasets/fisher" \
  --max_duration_in_seconds="20.0" \
  --min_duration_in_seconds="2.0" \
  --train_split="train_500" \
  --base_encoder_model="facebook/wav2vec2-xls-r-300m" \
  --feature_extractor_name="facebook/wav2vec2-xls-r-300m" \
  --base_decoder_model="gpt2" \
  --tokenizer_name="gpt2" \
  --enc_layers_to_freeze="24" \
  --steps_to_freeze_enc="-1" \
  --output_dir=$EXPERIMENT \
  --gradient_accumulation_steps="1" \
  --learning_rate="1e-5" \
  --logging_steps="10" \
  --save_strategy="steps" \
  --save_steps="1000" \
  --evaluation_strategy="steps" \
  --eval_steps="1000" \
  --auto_find_batch_size="True" \
  --per_device_train_batch_size="128" \
  --per_device_eval_batch_size="128" \
  --report_to="wandb" \
  --optim="adamw_torch" \
  --dataloader_num_workers="4" \
  --group_by_length="True" \
  --length_column_name="input_len" \
  --load_best_model_at_end="True" \
  --metric_for_best_model="eval_wer" \
  --early_stopping_patience="5" \
  --remove_unused_columns="False" \
  --predict_with_generate="True" \
  --save_total_limit="5" \
  --num_train_epochs=10 \
  --custom_optimizer="True" \
  --cross_attention_scaling_factor 10 \
  --num_beams="5" \
  --greater_is_better="False"
