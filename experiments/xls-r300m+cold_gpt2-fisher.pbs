#!/bin/bash
#PBS -q qgpu
#PBS -A OPEN-28-58
#PBS -l select=1:ncpus=32:ngpus=2
#PBS -l walltime=48:00:00
#PBS -N wav2vec2_pretrain

source /apps/all/Anaconda3/2022.05/bin/activate ~/.conda/envs/ec_asr/

SRC_DIR=/mnt/proj1/open-28-58/lakoc/LoCo-ASR
cd $SRC_DIR

export HF_HOME="${SRC_DIR}/huggingface_cache"
EXPERIMENT="XLS-R300m+cold_decoder_test"

WANDB_RUN_ID=$EXPERIMENT WANDB_PROJECT="LoCo-ASR" \
  python src/trainers/xlsr+gpt2.py \
  --dataset_name="${SRC_DIR}/datasets/fisher" \
  --max_duration_in_seconds="20.0" \
  --min_duration_in_seconds="2.0" \
  --train_subset="0.2" \
  --base_encoder_model="facebook/wav2vec2-xls-r-300m" \
  --feature_extractor_name="facebook/wav2vec2-xls-r-300m" \
  --base_decoder_model="gpt2" \
  --tokenizer_name="gpt2" \
  --enc_layers_to_freeze="24" \
  --steps_to_freeze_enc="-1" \
  --steps_to_freeze_dec="0" \
  --output_dir=$EXPERIMENT \
  --gradient_accumulation_steps="8" \
  --learning_rate="0.0003" \
  --logging_steps="50" \
  --save_strategy="steps" \
  --save_steps="10000" \
  --evaluation_strategy="steps" \
  --eval_steps="10000" \
  --auto_find_batch_size="True" \
  --per_device_train_batch_size="4" \
  --per_device_eval_batch_size="12" \
  --report_to="wandb" \
  --optim="adamw_torch" \
  --dataloader_num_workers="4" \
  --group_by_length="True" \
  --length_column_name="input_len" \
  --load_best_model_at_end="True" \
  --metric_for_best_model="eval_wer" \
  --remove_unused_columns="False" \
  --predict_with_generate="True" \
  --save_total_limit="5",

